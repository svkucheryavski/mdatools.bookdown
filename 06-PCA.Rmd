# Principal component analysis {-#pca}

In this chapter we will discuss how to use PCA method implemented in the *mdatools*. Besides that, we will use PCA examples to introduce some principles, which are common for most of the other methods (e.g. PLS, SIMCA, PLS-DA, etc.) available in this package. This includes such things as model and result objects, getting and visualization of performance statistics, validation, use of different kinds of plots, and so on.

Principal component analysis is one of the methods that decompose a data matrix $\mathbf{X}$ into a combination of three matrices: $\mathbf{X} = \mathbf{TP}^\mathrm{T} + \mathbf{E}$. Here $\mathbf{P}$ is a matrix with unit vectors, defined in the original variables space. The unit vectors, also known as *loadings*, form a new basis — *principal components*. The components are mutually orthogonal and oriented in variable space to capture direction of maximum variation of data points.

The data points are being projected to the principal components. Coordinates of these projections in principal component space, known as *scores*, forming matrix  $\mathbf{T} = \mathbf{XP}$. Product of scores and loadings, $\mathbf{TP}^\mathrm{T}$ gives coordinates of the projections in the original variable space. Matrix $\mathbf{E}$ contains residuals — difference between position of projected data points and their original locations. This difference is a part of data variation, which PCA model does not capture, hence the name.

If original data matrix has $I$ rows (observations, objects) and $J$ variables and PCA decomposition is made with $A$ components, then matrix $\mathbf{P}$ will have dimension $J\times A$, matrix $\mathbf{T}$ — $I\times A$, and $\mathbf{TP}^\mathrm{T}$ and $\mathbf{E}$ will have the same dimension as the original data matrix, $\mathbf{X}$.

Relationship between data objects and principal component space (PCA model) can be described using two distances (in some literature they are also called residual distances) — *orthogonal distance*, OD, and *score distance*, SD. The orthogonal distance is a squared Euclidean distance between the position of an object and its projection in original variable space. It can be computed by taking sum of squared values from matrix $\mathbf{E} = \{e_{ij}\}$ along every row:

$$q_i = \sum_{j=1}^{J} e_{ij}^2$$

This distance usually is denoted as $Q$ or $q$. It can be considered as a lack of fit measure for this particular object.

The score distance is a squared Mahalanobis distance between the projection of the objects and the origin. It is a measure of extremeness of object and usually denoted as $h$ or $T^2$. The latter is used because Hotelling $T^2$ distribution is often used to describe the distribution of the score distance values, so in many software the distance is called as *Hotelling $T^2$ distance*. The distance can be computed using standardized scores (so score values for every component have unit variance).

$$h_i = \sum_{a = 1}^{A} \frac{t_{ia}^2}{\lambda_a} $$

Here $\lambda$ are eigenvalues for the principal components, which correspond to the variance of corresponding scores.

Both score and orthogonal distances are important statistics allowing to assess how well objects are described by PCA model. They can be assessed visually, using the *Distance plot*, — scatter plot where orthogonal distance is plotted against the score distance for particular number of components. In *mdatools* this plot is called as *Residuals plot* due to some historical reasons.

Both distances can be described using theoretical distributions. This helps to identify regular and extreme objects as well as outliers. All details will be shown later in this tutorial.

There are several other methods, which can be used for decomposition of data similar to PCA. Some of them are Projection Pursuit (PP), Independent Component Analysis (ICA) and many others, that work in a similar way. The main difference among the methods is the way they find the orientation of the unit-vectors. Thus, PCA finds them as directions of maximum variance of data points. In addition to that, all PCA loadings are orthogonal to each other. The PP and ICA use other criteria for the orientation of the basis vectors and e.g. for ICA the vectors are not orthogonal.

There are several methods to compute PCA loadings, including Singular Values Decomposition (SVD) and Non-linear Iterative Partial Least Squares (NIPALS). Both methods are implemented in this package and can be selected using `method` argument of main class `pca`. By default SVD is used. In addition, one can use randomized version of the two methods, which can be efficient if data matrix contains large amount of rows. This is explained in the last part of this chapter.

## Models and results {-#pca--models-and-results}

In *mdatools*, any method for data analysis, such as PCA, PLS regression, SIMCA classification and so on, can create two types of objects — a model and a result. Every time you build a model you get a *model object*. Every time you apply the model to a dataset you get a *result object*. Thus for PCA, the objects have classes `pca` and `pcares` correspondingly.

Each object includes a list with object's properties (e.g. loadings for model, scores and explained variance for result) and provides a number of methods for visualization and exploration.

### Model calibration {-}

Let's see how this works using a simple example — *People* data. We already used this data when was playing with plots, it consists of 32 objects (persons from Scandinavian and Mediterranean countries, 16 male and 16 female) and 12 variables (height, weight, shoesize, annual income, beer and wine consumption and so on.). More information about the data can be found using `?people`. We will first load the data matrix and split it into two subsets as following:

```{r}
library(mdatools)
data(people)

idx = seq(4, 32, 4)
Xc = people[-idx, ]
Xt = people[idx, ]
```

So `Xc` is our calibration subset we are going to use for creating a PCA model and `Xt` is a subset we will apply the calibrated model to. Now let's calibrate the model:

```{r}
m = pca(Xc, 7, scale = TRUE, info = "People PCA model")
m = selectCompNum(m, 5)
```


Here `pca` is a function that builds (calibrates) a PCA model and returns the model object. In terms of programming we can call function `pca()` a constructor of pca model object. The constructor has one mandatory argument — matrix or data frame with data values. Other arguments are optional as they all have default values. In this case we use three additional: `7` is maximum number of components, `scale = TRUE` tells that data values should be standardized (centering is a default option) and `info` allows to add a short text with information about the model which can be shown later.

Function `selectCompNum()` allows to select an “optimal” number of components for the model. In our case we calibrate model with 7 principal components however, e.g. after investigation of explained variance, we found out that 5 components is optimal. In this case we have two choices. Either recalibrate the model using 5 components or use the model that is calibrated already but “tell” the model that 5 components is the optimal number. In this case the model will keep all results calculated for 7 components but will use optimal number of components when necessary. For example, when showing distance plot for the model. Or when PCA model is used in SIMCA classification.

Function `print` prints the model object info:

```{r}
print(m)
```

As you can see, there are no scores, explained variance values, distances and so on. Because they actually are not part of a PCA model, they are results of applying the model to a calibration set. But loadings, eigenvalues, number of calculated and selected principal components, vectors for centering and scaling the data, significance levels and corresponding critical limits for distances — all are parameters of the model.

Here is an example how to get values from loading matrix having the model object:

```{r}
m$loadings[1:4, 1:4]
```

One can also notice that the model object has a particular field — `res`, which is in fact a
list of PCA *result objects* containing results of applying the model to the calibration set (`cal`) and (if available) test set (`test`). In case of regression and classification `res` also contains object with cross-validation results (`cv`). For example here is the description of object with calibration results:

```{r}
print(m$res$cal)
```

And if we want to look at scores, here is the way:

```{r}
m$res$cal$scores[1:4, 1:4]
```

Both model and result objects also have related functions (methods), first of all for visualizing various values (e.g. scores plot, loadings plot, etc.). Some of the functions will be discussed later in this chapter, a full list can be found in help for a proper constructor (e.g. `?pca`).

The *result object* is also created every time you apply a model to a new data. Like in many built-in R methods, method `predict()` is used in this case. The first argument of the method is always a model object. Here is a PCA example (assuming we have already built the model):

```{r}
res = predict(m, Xt)
print(res)
```


### Model validation {-}

PCA model can be validated using only test set validation, while for supervised model, e.g. PLS or SIMCA, you can use cross-validation as well. The validation results are, of course, represented by *result objects*, which are fields of a *model object* similar to `cal`, but with names `test`.

Here is how to build a PCA model with test set validation (we will use `Xt` as test data):

```{r}
m = pca(Xc, 7, scale = TRUE, x.test = Xt, info = "PCA model")
m = selectCompNum(m, 5)
```

And here is the info for both result objects:

```{r}
print(m$res$cal)
print(m$res$test)
```

Let us compare, for example, the explained variance values for the results:

```{r}
var = data.frame(
   cal = m$res$cal$expvar,
   test = m$res$test$expvar
)
show(round(var, 1))
```

Every model and every result object has a method `summary()`, which shows some statistics for evaluation of a model performance. Here are some examples.

```{r}
# summary for whole model
summary(m)
```

```{r}
# summary for calibration results
summary(m$res$cal)
```

The same methodology is used for any other method, e.g. PLS or SIMCA. In the next section we will look at how to use plotting functions for models and results.

## Plotting methods {-#pca--plots}

First of all you can use the methods `mdaplot()` and `mdaplotg()` (or any others, e.g. `ggplot2`) for easy visualisation of the results as they are all available as matrices with proper names, attributes, etc. In the example below I create scores and loadings plots for PC1 vs PC2. Here I assume that the last model you have created was the one with test set validation, however scores are shown only for calibration set.

```{r, fig.width = 9, fig.height = 5}
par(mfrow = c(1, 2))
mdaplot(m$res$cal$scores, type = "p", show.labels = TRUE, show.lines = c(0, 0))
mdaplot(m$loadings, type = "p", show.labels = TRUE, show.lines = c(0, 0))
```

To simplify this routine, every model and result class also has a number of functions for visualization. Thus for PCA the function list includes scores and loadings plots, explained variance and cumulative explained variance plots, distance/residuals plots, and many others.

A function that does the same for different models and results has always the same name. For example, `plotPredictions` will show predicted vs. measured plot for PLS model and PLS result, MLR model and MLR result, SIMCA model and SIMCA result, and so on, although the plot itself will be quite different. The first argument must always be either a model or a result object.

The major difference between plots for model and plots for result is following. A plot for  result always shows one set of data objects — one set of points, lines or bars. For example, predicted vs. measured values for calibration set or scores values for test set and so on. For such plots method `mdaplot()` is used and you can provide any arguments, available for this method (e.g. color group scores for calibration results, add confidence ellipses for scores, etc.).

And a plot for a model in most cases shows several sets of data objects, e.g. predicted values for calibration and validation. In this case, a corresponding method uses `mdaplotg()` and, therefore, you can adjust the plot using arguments described for this method.

Here are some examples for results:

```{r, fig.width = 9, fig.height = 9}
# create a factor for combination of Sex and Region values
g1 <- factor(Xc[, "Sex"], labels = c("M", "F"))
g2 <- factor(Xc[, "Region"], labels = c("S", "M"))
g <- interaction(g1, g2)

# scores plot for calibration results colored by Height
par(mfrow = c(2, 2))
plotScores(m$res$cal, show.labels = TRUE, cgroup = Xc[, "Height"])

# scores plot colored by the factor created above and confidence ellipses
p = plotScores(m$res$cal, c(1, 2), cgroup = g)
plotConfidenceEllipse(p)

# distance plot for calibration results with labels
plotResiduals(m$res$cal, show.labels = TRUE)

# variance plot for calibration results with values as labels
plotVariance(m$res$cal, type = "h", show.labels = TRUE, labels = "values")
```

Scores plot can be also used together with `plotHotellingEllipse()` function. It works similar to `plotConfidenceEllipse()` or `plotConvexHull()` however does not require grouping of the values. You can add the ellipse both to score plot for results and score plot for model. In case of model, if you have e.g. both calibration and test set results you need to specify which one you want to use for the creating the ellipse. Code below shows several examples.

```{r, fig.width = 9, fig.height = 9}
par(mfrow = c(2, 2))

# default options for calibration results
p = plotScores(m$res$cal, xlim = c(-8, 8), ylim = c(-8, 8))
plotHotellingEllipse(p)

# also for calibration results but with specific significance limit and color
p = plotScores(m$res$cal, xlim = c(-8, 8), ylim = c(-8, 8))
plotHotellingEllipse(p, conf.lim = 0.9, col = "red")

# in case if several results objects are available, specify which plot series to use
# for creating the ellipse, in the example below we take the first plot series which
# corresponds to calibration set
p = plotScores(m, xlim = c(-8, 8), ylim = c(-8, 8))
plotHotellingEllipse(p[[1]])

# same but with specific settings for color and line
p = plotScores(m, xlim = c(-8, 8), ylim = c(-8, 8))
plotHotellingEllipse(p[[1]], col = "red", lty = 1, lwd = 0.5)

```

The color grouping option is not available for the group (model) plots as colors are used there to underline the groups.

Now let's look at similar plots (plus loadings) for a model.

```{r, fig.width = 9, fig.height = 9}
par(mfrow = c(2, 2))
plotScores(m, c(1, 3), show.labels = TRUE)
plotLoadings(m, c(1, 3), show.labels = TRUE)
plotResiduals(m, show.labels = TRUE, col = c("green", "red"))
plotVariance(m, type = "h", show.labels = TRUE, labels = "values")
```

As you can see, for in case of model, the distance plot also shows some lines. These are critical limits, all details about them as well as how to use the distance/residuals plot will be given in the next section.

If model plot show values from calibration and validation objects (e.g. scores, distances, etc), you can specify which result to show. In the example below left plot is made using default settings, for the right plot we specified list with results explicitly. Note, that the results should be specified as a named list.

```{r, fig.width = 9, fig.height = 5}
par(mfrow = c(1, 2))
plotResiduals(m)
plotResiduals(m, res = list("train" = m$res$cal))
```

Finally, method `plot()` shows the main four PCA plots as a model (or results) overview.

```{r, fig.width = 9, fig.height = 9}
plot(m, show.labels = TRUE)
```

You do not have to care about labels, names, legend and so on, however if necessary you can always change almost anything. See full list of methods available for PCA by `?pca` and `?pcares`.

### Support for images {-}

As it was described before, images can be used as a source of data for any methods. In this case the results, related to objects/pixels will inherit all necessary attributes and can be show as images as well. In the example below we make a PCA model for the image data from the package and show scores and distances.

```{r, fig.width = 9, fig.height = 9}
data(pellets)
X = mda.im2data(pellets)
m = pca(X)

par(mfrow = c(2, 2))
imshow(m$res$cal$scores)
imshow(m$res$cal$Q)
imshow(m$res$cal$scores, 2)
imshow(m$res$cal$Q, 2)
```

### Manual x-values for loading line plot {-}

As it was discussed in the previous chapter, you can specify a special attribute, `"xaxis.values"` to a dataset, which will be used as manual x-values in bar and line plots. When we create any model and/or results the most important attributes, including this one, are inherited. For example when you make a loading line plot it will be shown using the attribute values.

Here is an example that demonstrate this feature using PCA decomposition of the *Simdata* (UV/Vis spectra).

```{r, fig.width = 9, fig.height = 5}
data(simdata)

X = simdata$spectra.c
attr(X, "xaxis.name") = "Wavelength, nm"
attr(X, "xaxis.values") = simdata$wavelength

m = pca(X, 3)
plotLoadings(m, 1:2, type = "l")
```

### Excluding rows and columns {-}

PCA as well as any other method in `mdatools` can exclude rows and columns from calculations. For example, it can be useful if you have some candidates for outliers or do variable selection and do not want to remove rows and columns from the data matrix. In this case you can just specify two additional parameters, `exclcols` and `exclrows`, using either numbers or names of rows/columns to be excluded. You can also specify a vector with logical values (all `TRUE`s will be excluded).

The excluded rows are not used for creating a model and calculation of model's and results' performance (e.g. explained variance). However main results (for PCA — scores and residual distances) are calculated for these rows as well and set hidden, so you will not see them on plots. You can always show values for excluded objects by using parameter `show.excluded = TRUE`. It is implemented via attributes "known" for plotting methods from *mdatools* so if you use e.g. *ggplot2* you will see all points.

The excluded columns are not used for any calculations either, the corresponding results (e.g. loadings or regression coefficients) will have zero values for such columns and be also hidden on plots. Here is a simple example.

```{r, fig.width = 9, fig.height = 9}
data(people)

m = pca(people, 5, scale = T, exclrows = c("Lars", "Federico"), exclcols = "Income")

par(mfrow = c(2, 2))
plotScores(m, show.labels = TRUE)
plotScores(m, show.labels = TRUE, show.excluded = TRUE)
plotResiduals(m, show.labels = TRUE, show.excluded = TRUE)
plotLoadings(m, show.labels = TRUE, show.excluded = TRUE)
```

As you can see, the excluded values (or variables in case of loadings plot) are either hidden completely or shown as gray points if `show.excluded` parameter is set to `TRUE`.

Here is the matrix with loadings, note that variable `Income` has zeros for loadings and the matrix has attribute `exclrows` set to 6 (which is a position of the variable):

```{r}
# show matrix with loadings (look at row Income and attribute "exclrows")
show(m$loadings)
```

Such behavior will help to exclude and include rows and columns interactively, some examples will be shown later.

## Distances and critical limits {-#pca--distances-and-limits}

### Distance to model and score distance {-}

As it was written in the brief theoretical section about PCA, when data objects are being projected to a principal component space, two distances are calculated and stored as a part of PCA results (`pcares` object). The first is a squared orthogonal Euclidean distance from original position of an object to the PC space, also known as *orthogonal distance* and denoted as *Q* or *q*. The distance shows how well the object is fitted by the PCA model and allows to detect objects that do not follow a common trend, captured by the PCs.

The second distance shows how far a projection of an object to the PC space is from the origin. This distance is also known as Hotelling T^2^ distance or a *score distance*. To compute T^2^ distance scores should be first normalized by dividing them to corresponding singular values (or eigenvalues if we deal with squared scores). The T^2^ distance allows to see extreme objects — which are far from the origin. In the package as well as in this tutorial we will also use letter $h$ to denote this distance, so $Q$ and $q$ are for orthogonal distance and $T^2$ and $h$ stand for the score distance.

```{block, type='caution'}
Historically, the distances are called in this package and also in some other software as *Residual distances* or just *residuals*. Therefore functions, which visualize the corresponding distances, are called `plotResiduals()` (`plotXResiduals()`, `plotYResiduals()`, `plotXYResiduals()` in PLS based methods). However, this is not fully correct, and in the text of the tutorial we will refer to them as *distances* most of the time. Changing function names though will lead to large degree of incompatibility.
```

In *mdatools* both distances are calculated for all objects in dataset and all possible components. So every distance is represented by $I \times A$ matrix (will remind here that $I$ is number of objects or rows in data matrix and $A$ is number of components in PCA model), the first column contains distances for a model with only one component, second — for model with two components, and so on. The distances can be visualized using method `plotResiduals()` which is available both for PCA results as well as for PCA model. In the case of model the plot shows distances both for calibration set and test set (if available).

Here is an example, where I split People data to calibration and test set as in one of the examples in previous sections and show distance plot for model and result objects.

```{r, fig.width = 9, fig.height = 9}
data(people)
idx = seq(4, 32, 4)
Xc = people[-idx, ]
Xt = people[idx, ]

m = pca(Xc, 4, scale = TRUE, x.test = Xt)

par(mfrow = c(2, 2))
plotResiduals(m, main = "Distances for model")
plotResiduals(m, ncomp = 2, main = "Distances for model (2 Comp)")
plotResiduals(m$res$cal, main = "Distances for calibration set")
plotResiduals(m$res$test, main = "Distances for test set")
```

As it was briefly mentioned before, if residual/distance plot is made for a model, it also shows two critical limits: the dashed line is a limit for extreme objects and the dotted line is a limit for outliers. How they are computed is explained later in this section. If you want to hide them, just use additional parameter `show.limits = FALSE`. You can also change the color, line type and width for the lines by using options `lim.col`, `lim.lty` and `lim.lwd` as it is shown below.

```{r, fig.width = 9, fig.height = 5}
par(mfrow = c(1, 2))
plotResiduals(m, show.limits = FALSE)
plotResiduals(m, lim.col = c("red", "orange"), lim.lwd = c(2, 2), lim.lty = c(1, 2))
```

It is necessary to provide a vector with two values for each of the argument (first for extreme objects and second for outliers border).

Finally, you can also notice that for model plot the distance values are normalized ($h/h_0$ and $q/q_0$ instead of just $h$ and $q$). This is needed to show the limits correctly and, again, will be explained in detail below. You can switch this option by using logical parameter `norm`. Sometimes, it is also make sense to use log transform for the values, which can be switched by using parameter `log`. Code below shows example for using these parameters.

```{r, fig.width = 9, fig.height = 9}
par(mfrow = c(2, 2))
plotResiduals(m, norm = FALSE)
plotResiduals(m, norm = TRUE)
plotResiduals(m, norm = TRUE, log = TRUE)
plotResiduals(m, norm = FALSE, log = TRUE)
```

### Critical limits {-}

If PCA model is made for dataset taken from the same population, the orthogonal and score distances can be used to find outliers and extreme objects. One of the ways to do this is to compute critical limits for the distances assuming that they follow certain theoretical distribution.

Critical limits are also important for SIMCA classification as they are directly used for making decision on class belongings. This package implements several methods to compute the limits, which are explained in this section.

#### Data driven approach {-}

Starting from version 0.10.0, by default the limits are computed using data driven approach  [proposed](https://onlinelibrary.wiley.com/doi/abs/10.1002/cem.1147) and then [extended](https://onlinelibrary.wiley.com/doi/abs/10.1002/cem.2506) by Pomerantsev and Rodionova.

It was known before, that both distances are well described by chi-square distribution. The distribution in general describes behavior of sum of squared random values taken from standardized normal distribution, which means, that the distances has to be standardized first. The scaling factor as well as number of degrees of freedom (DoF) can be estimated from the distance values — hence the name (data driven).

Since the estimation procedure is identical for the both distances, we will use a generalized version. Let's say we have a vector of distance values $u$ (where $u$ can be either score distance values, $h$, or orthogonal distance values, $q$) for given number of components. Then the scaling factor, $u_0$ and corresponding DoF, $N_u$ can be found as:

$$u_0 = m_u = \frac{1}{I} \sum_{i = 1}^{I} u$$
$$N_u = \mathrm{int} \bigg( \frac{2u_0^2}{s_u^2} \bigg)$$

Here $s_u^2$ is a variance of the distances. Then the the normalized distance values will follow chi-square distribution with $N_u$ degrees of freedom (just replace $u$ to $q$ or $h$ to get the formula for particular distance):

$$N_u \frac{u}{u_0} \propto \chi^2(N_u)$$

However, this does not take into account the fact that $h$ and $q$ are, strictly speaking, not independent. It is well known that adding an additional component to PCA model leads to increase of score distance and decrease of orthogonal distance and removing a component has the opposite effect. This relationship can be taken into account by computing a joint or a *full distance*, $f$, as follows:

$$f = N_h \frac{h}{h_0} + N_q \frac{q}{q_0}$$

The full distance, $f$ also follows chi-square distribution with degrees of freedom: $N_f = N_q + N_h$. The critical limits for the full distance are computed using inverse cumulative distribution function (ICDF) also known as quantile function. For extreme objects (shown on distance plot as dashed line) as:

$$f_{crit} = \chi^{-2}(1 - \alpha, N_f)$$

Where $\alpha$ is a significance level — expected number of extreme objects (e.g. for $\alpha = 0.05$ we expect that 5% of objects will be categorized as extreme). Critical limit for outliers is found as:

$$f_{crit} = \chi^{-2}((1 - \gamma)^{1/I}, N_f)$$

Here $\gamma$ is a significance level for outliers, so if it is $0.01$ every object has $1\%$ chance to be detected as outlier. The test for outliers should treat every object independently, so it requires Bonferroni correction, as one can see from the formula.

You can change the significance level both for extreme objects and outliers either when you calibrate the model, by providing parameters `alpha` and `gamma`, or for existent model, by using function `setDistanceLimits()` as shown in the example below.

```{r, fig.width = 9, fig.height = 5}
# calibrate a model with alpha = 5% and gamma = 5%
m = pca(people, 4, scale = TRUE, alpha = 0.05, gamma = 0.05)

par(mfrow = c(1, 2))
plotResiduals(m)

# change both levels to 1%
m = setDistanceLimits(m, alpha = 0.01, gamma = 0.01)
plotResiduals(m)
```

This function also allows to change method for computing the limits, which is discussed in next subsection.

#### Other methods for computing critical limits {-}

The package implements several other methods for computing critical limits for residuals distances. The needed method can be selected by providing additional argument, `lim.type`, both when calibrate PCA model or when adjust the limits for existent model with `setDistanceLimits()` function. By default, `lim.type = "ddmoments"`, which corresponds to the method described above — data driven approach based on classical estimators (statistical moments).

When data is contaminated with outliers, using statistical moments may lead to wrong estimators, in this case it is recommended to use robust version instead by specifying `lim.type = "ddrobust"`. The robust approach utilizes median and inter-quartile range instead of mean and standard deviation (see [the paper](https://onlinelibrary.wiley.com/doi/abs/10.1002/cem.2506) for details). The use of robust and classical data driven approaches also helps to identify a proper model complexity, which will be discussed in the next section.

In addition to the data driven method, *mdatools*, also allows to use state-of-art approach, where limits for each distance are computed independently giving rectangular acceptance area on the distance plot. In this case, two methods are available for the orthogonal distance: either based on chi-square distribution but only for $q$ values (`lim.type="chisq"`) or using Jackson-Mudholkar method (`lim.type="jm"`). If one of the two methods is selected, critical limits for the score distances, $h$, will be computed using [Hotelling's T^2^ distribution](https://en.wikipedia.org/wiki/Hotelling%27s_T-squared_distribution). This way of computing critical limits can be found in many popular chemometric software, for example, PLS_Toolbox.

#### Categorization of data rows based on critical limits {-#pca-distances-categorize}

It is possible to categorize every row (object, measurement) in a dataset based on the two distances and critical limits computed for given model and limit parameters (method and significance limit). Function `categorize()` is made for that. An example below shows how to categorize new predictions using this function.

```{r}
data(people)

idx = seq(4, 32, 4)
Xc = people[-idx, ]
Xt = people[idx, ]

m = pca(Xc, 4, scale = TRUE)
r = predict(m, Xt)

c = categorize(m, r)
print(c)
```

As one can see, there are two outliers, one extreme and five regular objects. This can be visualized on distance plot:

```{r, fig.width = 9, fig.height = 5}
par(mfrow = c(1, 2))
plotResiduals(r, cgroup = c)
plotResiduals(m, res = list("new" = r), cgroup = c)
```

The first plot is a normal distance/residuals plot for results, while the second plot is made for model with manually specified result list. The last has possibility to show critical limits and see why actually the objects have been categorized like this.

When the plot is made for calibration set, this option can be simplified by providing specific value for parameter `cgroup` as shown below.

```{r, fig.width = 9, fig.height = 5}
par(mfrow = c(1, 2))
plotResiduals(m)
plotResiduals(m, cgroup = "categories")
```

This option will work only if plot is made for one result object.

## Model complexity {-#pca--model-complexity}

Complexity of PCA model is first of all associated with selection of proper (optimal) number of components. The optimal number should explain the systematic variation of the data points and keep the random variation uncaptured. Traditionally, number of components is selected by investigation of eigenvalues or looking at plot with residual or explained variance. However, this is quite complicated issue and result of selection depends very much on quality of data and purpose PCA model is built for. More details can be found in [this paper](https://doi.org/10.1016/j.chemolab.2021.104304).

In *mdatools* there are several additional instruments both to select proper number of components as well as to see if, for example, new data (test set or new set of measurements) are well fitted by the model.

The first tool is the use of degrees of freedom for orthogonal and score distances, we mentioned earlier. A simple plot, where number of the degrees of freedom is plotted against number of components in PCA model can often reveal the presence of overfitting very clearly — the DoF value for the orthogonal distance, $N_q$ jumps up significantly.

The code and its outcome below show how to make such plot separately for each distance and for both. The first plot in the figure is conventional plot with variance explained by each component. The model is made for *Simdata*, where it is known that the optimal number of components is two.

```{r, fig.width = 9, fig.height = 9}
data(simdata)
Xc = simdata$spectra.c
Xt = simdata$spectra.t
m = pca(Xc, 6)

par(mfrow = c(2, 2))
plotVariance(m, type = "h", show.labels = TRUE)
plotQDoF(m)
plotT2DoF(m)
plotDistDoF(m)
```

As you can see from the plots, second component explains less than 1% of variance and can be considered as non significant. However, plot for $N_q$ shows a clear break at $A = 3$, indicating that both first and second PCs are important. The $N_h$ values in this case do not provide any useful information.

In case if outliers present it can be useful to investigate the plot for $N_q$ using classical and robust methods for estimation as shown in example below.

```{r, fig.width = 9, fig.height = 5}
m = pca(Xc, 6)

par(mfrow = c(1, 2))
plotQDoF(m, main = "DoF plot (classic)")

m = setDistanceLimits(m, lim.type = "ddrobust")
plotQDoF(m, main = "DoF plot (robust)")
```

In this case both plots demonstrate quite similar behavior, however if they look differently it can be an indicator for a presence of outliers. The DoF plots work only if data driven method is used for computing of critical limits.

Another way to see how many components are optimal in the model, in case if you have a test set or just a new set of measurements you want to use the model with, is to employ the Extreme plot. The plot shows a number of extreme values for different $\alpha$. Imaging that you make several distance plots with different $\alpha$ values and count how many objects model found as extremes. On the other hand you know that the expected value is $\alpha I$. If you plot the number of extreme values vs. the expected number — this is the Extreme plot.

If model captures systematic variation both for calibration and test set the points on this plot will lie within a confidence ellipse shown using light blue color. However, if model is overfitted, the points are getting outside it.

Below you can see two Extreme plots made for the same data as the previous example. The left plot show results for calibration set for number of components in the PCA model equal to 2 and 3. The right plot show the results for the test set.

```{r, fig.width = 9, fig.height = 5}
Xc = simdata$spectra.c
Xt = simdata$spectra.t
m = pca(Xc, 6, x.test = Xt)

par(mfrow = c(1, 2))
plotExtreme(m, comp = 2:3, main = "Extreme plot (cal)")
plotExtreme(m, comp = 2:3, res = m$res$test, main = "Extreme plot (test)")
```

As you can see, in case of calibration set, the points are lying withing confidence ellipse for both $A = 2$ and $A = 3$. However for the test set, the picture is quite different. In case of $A = 2$ most of the points are inside the interval, but for $A = 3$ all of them are clearly outside.

We hope these new tools will make the use of PCA more efficient.

## Randomized PCA algorithms {-#pca--randomized-algorithm}

Both SVD and NIPALS are not very efficient when number of rows in dataset is very large (e.g. hundreds of thousands values or even more). Such datasets can be easily obtained in case of for example hyperspectral images. Direct use of the traditional algorithms with such datasets often leads to a lack of memory and long computational time.

One of the solution here is to use probabilistic algorithms, which allow to reduce the number of values needed for estimation of principal components. Starting from *0.9.0* one of the probabilistic approach is also implemented in *mdatools*. The original idea can be found in [this paper](https://epubs.siam.org/doi/10.1137/090771806) and some examples on using the approach for PCA analysis of hyperspectral images are described [here](https://onlinelibrary.wiley.com/doi/abs/10.1002/cem.2966).

The approach is based on calculation of matrix $B$, which has much smaller number of rows comparing to the original data, but captures the action of the data. To produce proper values for $B$, several parameters are used. First of all it is a rank of original data, $k$, if it is known. Second, it is oversampling parameter, $p$, which is important if rank is underestimated. The idea is to use $p$ to overestimate the rank thus making solution more stable. The third parameter, $q$, is a number of iterations needed to make $B$ more robust. Usually using $p = 5$ and $q = 1$ will work well on most of the datasets and, at the same time, will take less time for finding a solution comparing with conventional methods. By default, $k$ is set to number of components, used for PCA decomposition.

The example below uses two methods (classic SVD and randomized SVD) for PCA decomposition of 100 000 x 300 dataset and compares time and main outcomes (loadings and explained variance).

```{r}
# create a dataset as a linear combination of three sin curves with random "concentrations"
n = 100000
X = seq(0, 29.9, by = 0.1)
S = cbind(sin(X), sin(10 * X), sin(5 * X))
C = cbind(runif(n, 0, 1), runif(n, 0, 2), runif(n, 0, 3))
D = C %*% t(S)
D = D + matrix(runif(300 * n, 0, 0.5), ncol = 300)
show(dim(D))

# conventional SVD
t1 = system.time({m1 = pca(D, ncomp = 2)})
show(t1)

# randomized SVD with p = 5 and q = 1
t2 = system.time({m2 = pca(D, ncomp = 2, rand = c(5, 1))})
show(t2)

# compare variances
summary(m1)
summary(m2)

# compare loadings
show(m1$loadings[1:10, ])
show(m2$loadings[1:10, ])

```

As you can see the explained variance values, eigenvalues and loadings are identical in the two models and the second method is several times faster.

It is possible to make PCA decomposition even faster if only loadings and scores are needed. In this case you can use method `pca.run()` and skip other steps, like calculation of distances, variances, critical limits and so on. But in this case data matrix must be centered (and scaled if necessary) manually prior to the decomposition. Here is an example using the data generated in previous code.

```{r}
D = scale(D, center = TRUE, scale = FALSE)

# conventional SVD
t1 = system.time({m1 = pca.run(D, method = "svd", ncomp = 2)})
show(t1)

# randomized SVD with p = 5 and q = 1
t2 = system.time({m2 = pca.run(D, method = "svd", ncomp = 2, rand = c(5, 1))})
show(t2)

# compare loadings
show(m1$loadings[1:10, ])
show(m2$loadings[1:10, ])

```

As you can see the loadings are still the same but the probabilistic algorithm is about 10 times faster. The exact difference depends on computer of course so if you run these examples on your compute you will get a bit different results, however the randomized algorithm will always be a clear winner.

## Procrustes cross-validation {-#pca--pcv}

Procrustes cross-validation (PCV) is a new approach for validation of PCA and SIMCA models. You can find detailed description of the method with numerous examples in [this paper](https://doi.org/10.1021/acs.analchem.0c02175).

PCV makes possible to generate a new dataset, named *pseudo-validation set* and use it for validation of models in the same way as with an independent test set. The generation is done using following algorithm:

1. A global PCA model is created using calibration set **X** and *A* components
2. The rows of matrix **X** are split into *K* segments using venetian blinds splitting
3. For each segment *k* from *{1, 2, ..., K}*:
    * a local PCA model is created using the rows from the segment
    * an angle between the local and the global model is estimated
    * rows from the current segment are rotated in original variable space by the angle
4. All rotated measurements are then combined into a matrix with pseudo-validation set

So, pseudo-validation set is built on top of the calibration set but has its own sampling error. Since it is not independent from the calibration set we recommend to limit its use by model optimization and do not use it for assessment of performance of the final model.

The syntax is quite simple:

```{r, eval = FALSE}
pvset <- pcv(X, ncomp, nseg, scale)
```

Here `X` is your calibration set (as a numerical matrix), `ncomp` is a number of principal components for PCA decomposition (use at lease enough to explain 95-99% of variation of the data values), `nseg` is the number of segments for cross-validation procedure. So far only systematic cross-validation (venetian blinds) is supported, so make sure that rows of `X` are sorted correctly or shuffled. Parameter `scale` allows to standardize your data prior to the generation, which is useful if your variables have different nature and/or units. The generated data will be unscaled though.

The code below show an example, where we generate pseudo-validation set for spectra from the *Simdata*. The plots show spectra from both sets.

```{r, fig.width = 9, fig.height = 9}
data(simdata)
spectra = simdata$spectra.c

# create pseudo-validation set
spectra_pv = pcv(spectra, ncomp = 6, nseg = 4)

# show plot with original and generated spectra
par(mfrow = c(2, 1))
mdaplot(spectra, type = "l", main = "Original")
mdaplot(spectra_pv, type = "l", main = "Pseudo-validation")
```

On one hand, the plots seem very similar, almost identical. However this is not true. Let's use the generated pseudo-validation set as a test set. For example, we can make an Extreme plot and find out that fourth component leads to overfitted model. Note, that if we make this plot for the calibration set (left), we do not see any signs of overfitting.

```{r, fig.width = 9, fig.height = 5}
# make PCA model for calibration set
m = pca(spectra, 6)

# project pseudo-validation set to the model
res_pv = predict(m, spectra_pv)

# show extreme plot for A = 4
par(mfrow = c(1, 2))
plotExtreme(m, res = m$res$cal, comp = 4, main = "Extreme (cal)")
plotExtreme(m, res = res_pv, comp = 4, main = "Extreme (pv)")
```