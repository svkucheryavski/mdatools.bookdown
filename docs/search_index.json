[
["index.html", "Getting started with mdatools for R Introduction", " Getting started with mdatools for R Sergey Kucheryavskiy September 12, 2016 Introduction This is a user guide for mdatools — R package for preprocessing, exploring and analysis of multivariate data. The package provides methods mostly common for Chemometrics. The general idea of the package is to collect the popular chemometric methods and give a similar “user interface” for using them. So if a user knows how to make a model and visualize results for one method, he or she can easily do this for the other methods as well. I decided to rewrite this tutorial in order to add more details and examples for the implemented methods. This is a pre-release version and I am still working on it. So far it only presents how to work with datasets and plots as well as describes in details how to do PCA analysis. More chapters (PLS, variable selection, classification) will be available soon. You can track main changes in the text here. All methods implemented in the package were tested using well-known datasets. However, there still could be some bugs, in this case please report to svkucheryavski@gmail.com or use Issues tool at GitHub. You are also very welcome to share your comments and suggestions about the package functionality. "],
["what-is-new.html", "What is new", " What is new 09.09.2016 A new version (0.8.0) brings a lot of new features, therefore it was decided to rewrite this tutorial completely and start this log from the scratch. Most of the things available in the previous version of the package will work without any changes. But if you have been using functions mdaplot() and mdaplotg() it makes sense to read how the new implementation works and rewrite your code. The use of plotting tools became much simpler and more efficient. The main changes in the package are: added a possibility to assign some specific attributes to datasets, which makes plotting and some other operations easier added a possibility to exclude (hide) specific rows and columns from calculations without removing them physically via methods mda.exclrows(), mda.exclcols() if a data frame has factor columns they will be automatically converted to a set of dummy variables added several functions to make the operations with datasets containing specific attributes easier (mda.rbind(), mda.cbind(), mda.t(), mda.subset(), mda.show()) plotting tools (mdaplot(), mdaplotg()) were rewritten to make the use of them easier and more efficient most of the model and result specific plots now have additional options, e.g. you can use wavelength or wavenumbers when show plots for loadings, regression coefficients, etc. scores and loadings plots now show a percent of explained variance added support for images, see a specific chapter for details cross-validation procedure were optimized for most of the methods and now takes less time several bug fixes and small improvements "],
["overview.html", "Overview", " Overview This package was created for an introductory PhD course on Chemometrics given at Department of Chemistry and Bioscience, Aalborg University. Quickly I found out that using R for this course (with all advantages it gives) needs a lot of routine work from students, since most of them were also beginners in R. Of course it is very good for understanding when students get to know e.g. how to calculate explained variance or residuals in PCA manually or make corresponding plots and so on, but for the introductory course these things (as well as numerous typos and small mistakes in a code) take too much time, which can be spent for explaining methods and proper interpretation of results. This is actually also true for everyday using of these methods, most of the routines can be written ones and simply reused with various options. So it was decided to write a package where most widely used chemometric methods for multivariate data analysis are implemented and which gives also a quick and easy-to-use access to results, produced by these methods. First of all numerous plots. Here how it works. Say, we need to make a PCA model for data matrix x with autoscaling. Then make an overview of most important plots and investigate scores and loadings for first three components. The mdatools solution will be: # make a model for autoscaled data with maximum possible number of components m = pca(x, scale = TRUE) # show explained variance plot plotVariance(m) # select optimal number of components (say, 4) for correct calculation of residuals m = selectCompNum(m, 4) # show plots for model overview plot(m) # show scores plot for PC1 and PC3 plotScores(m, c(1, 3)) # show loadings plot for the same components plotLoadings(m, c(1, 3)) # show the loadings as a set of bar plots plotLoadings(m, c(1, 3), type = &#39;h&#39;) Fairly simple, is not it? The other “routine”, which have been taken into account is validation — any model can be cross-validated or validated with a test set. The model object will contain the validation results, which will also appear on all model plots, etc. See the next chapters for details. "],
["what-mdatools-can-do.html", "What mdatools can do?", " What mdatools can do? The package includes classes and functions for analysis, preprocessing and plotting data and results. So far the following methods for analysis are implemented: Principal Component Analysis (PCA) Soft Independent Modelling of Class Analogy (SIMCA) Partial Least Squares regression (PLS) with calculation of VIP scores and Selectivity ratio Partial Least Squares Discriminant Analysis (PLS-DA) Randomization test for PLS regression models Interval PLS for variable selection Preprocessing methods include: Mean centering, standardization and autoscaling Savitzky-Golay filter for smoothing and derivatives Standard Normal Variate for removing scatter effect from spectral data Mutliplicative Scatter Correction for the same issue Normalization of spectra to unit area or unit length More methods both for analysis and preprocessing are coming. Besides that, some extensions for the basic R plotting functionality have been also implemented and allow to do the following: Color grouping of objects with automatic color legend bar. Plot for several groups of objects with automatically calculated axes limits and plot legend. Two built-in color schemes — one is based on Colorbrewer and the other one is a grayscale scheme. Very easy-to-use possibility to apply any user defined color scheme. Possibility to show horizontal and vertical lines on the plot with automatically adjusted axes limits. Possibility to extend plotting functionality by using some attributes for datasets. See ?mdatools and next chapters for more details. "],
["how-to-install.html", "How to install", " How to install The package is available from CRAN by usual installing procedure. However due to restrictions in CRAN politics regarding number of submissions (once in 3-4 month) only major releases will be published there. To get the latest release plase use GitHub sources. You can either download a zip-file with the source package and install it using the install.packages command, e.g. if the downloaded file is mdatools_0.8.0.tar.gz and it is located in a current working directory, just run the following: install.packages(&#39;mdatools_0.8.0.tar.gz&#39;) If you have devtools package installed, the following command will install the latest release from the GitHub (do not forget to load the devtools package first): install_github(&#39;svkucheryavski/mdatools&#39;) -->"],
["datasets-and-plots.html", "Datasets and plots", " Datasets and plots The package uses standard representation of the data in R: data frames, matrices and vectors. However, there are several additional methods and attributes, which make the use of the datasets a bit more more efficient. There is also a support for images. But if you are used to simple datasets and standard procedures and do not want any complications, you can simply skip this chapter. The package also uses its own set of plotting tools, which is a sort of an add-on for the R basic plotting system, extending its possibilities. From this point of view, learning how these tools work will simplify understanding of model plots a lot. The main improvements comparing to the basic plotting system are: Much easier way to make plots with groups of objects (points, lines, bars, etc.) Much easier way of adding legend to the group plots. Much easier way of adding labels to data points, bars, etc. Automatic axis limits when a plot contains several groups of objects. Possibility to color points and lines according to values of a specific numerical variable of a factor. Two built in color pallets and an easy way to use user specific set of colors. Much more! This chapter explains most of the details. "],
["attributes-and-factors.html", "Attributes and factors", " Attributes and factors This section tells how to extend the functionality of the package by using attributes assigned to datasets and how the implemented methods deal with factors. Package specific attributes There are several groups of attributes, which allow to assign names and manual x-values to the datasets, exclude columns and rows from calculations without removing them physically as well as working with images. We will illustrate how to work with most of the attributes by using a simple dataset defined below. It consists of three variables Height, Weight, Shoesize and four records/objects/rows. d = matrix(c(190, 180, 170, 175, 85, 88, 70, 75, 44, 45, 38, 40), ncol = 3) colnames(d) = c(&#39;Height&#39;, &#39;Weight&#39;, &#39;Shoesize&#39;) rownames(d) = c(&#39;Bob&#39;, &#39;Peter&#39;, &#39;Lisa&#39;, &#39;Lena&#39;) d = as.data.frame(d) show(d) ## Height Weight Shoesize ## Bob 190 85 44 ## Peter 180 88 45 ## Lisa 170 70 38 ## Lena 175 75 40 Attributes for plots These attributes will be explained very briefly here, you can find much more details in the next two sections. The idea is to provide some names and values to the data, which can be used later e.g. for making labels and titles on the plots. When dataset is used to create a model (e.g. PCA) all results representing objects (e.g. scores, Q-residuals, T2-residuals, etc.) will inherit the row specific attributes and all results representing objects (e.g. loadings) will inherit column specific attributes. The attributes are following: Attribute Meaning name name of a dataset (used for plot parameter main) xaxis.name name for all data columns (used for plot parameter xlab) yaxis.name name for all data rows (used for plot parameter ylab) xaxis.values a vector of values, which correspond to the columns (e.g. for spectroscopic data it can be wavelength or wavenumbers) yaxis.values a vector of values, which correspond to the rows (e.g. for kinetic data it can be time or temperature of reaction) Here is a very simple example. attr(d, &#39;name&#39;) = &#39;People&#39; attr(d, &#39;xaxis.name&#39;) = &#39;Parameters&#39; attr(d, &#39;yaxis.name&#39;) = &#39;Persons&#39; par(mfrow = c(1, 2)) mdaplot(d, type = &#39;p&#39;) mdaplot(d, type = &#39;l&#39;) See more details in the section about plots. Excluding/hiding rows and columns When we are working with models, one of the most common procedures is hiding outliers and irrelevant or not important variables. It can be done simply by removing corresponding rows and columns. However the more flexible way will be just to mark them as excluded, therefore it was decided to create some tools to make this part of data analysis a bit easier. The main idea is very simple. Any dataset (matrix or a data frame) may have two attributes: exclrows for rows that should not be used when model is created and exclcols — similar parameter for columns. The simplest way to excluded rows or columns is to provide number, names or a vector with logical values as a value of the attributes. However, especially when removing items is a sequential procedure and there are no names for rows or/and variables, it will be much easier to use the specific methods: mda.exclrows() and mda.exclcols(). Both functions take row or column numbers, names or a vector with logical values and convert them to indices taking into account the previously excluded objects. Here is an example of two step procedure for hiding/excluding objects using these methods. We will use another package specific method, mda.show(), which hides the excluded elements. Let’s start with showing the original dataset again. mda.show(d) ## People ## ------ ## Height Weight Shoesize ## Bob 190 85 44 ## Peter 180 88 45 ## Lisa 170 70 38 ## Lena 175 75 40 Now we will hide all rows with Height above 180 cm (actually this is only one row). d = mda.exclrows(d, d$Height &gt; 180) mda.show(d) ## People ## ------ ## Height Weight Shoesize ## Peter 180 88 45 ## Lisa 170 70 38 ## Lena 175 75 40 Then we exclude the second row from the available data. d = mda.exclrows(d, 2) mda.show(d) ## People ## ------ ## Height Weight Shoesize ## Peter 180 88 45 ## Lena 175 75 40 Show indices of the excluded rows. Note that the second row we excluded in the last step, was third in the original data and this index is kept in the attributes. attr(d, &#39;exclrows&#39;) ## [1] 1 3 We can also unhide the rows using the indices. d = mda.inclcols(d, c(1, 3)) mda.show(d) ## People ## ------ ## Height Weight Shoesize ## Peter 180 88 45 ## Lena 175 75 40 Working with columns is similar, but in this case you need to use method mda.exclcols(). What happen to the excluded items when one calibrates or apply a model, e.g. PCA? In this case the outcome, e.g. scores and loadings will correspond to the original size of the data, but: Loadings will be computed without hidden objects and variables Matrix with loadings will have zero values for the hidden variables and the corresponding columns will be also marked as excluded. Matrix with scores will have score values calculated for the hidden objects but the rows will be excluded. So you can always see the scores and e.g. residuals for the excluded objects if necessary. More details will be shown in the PCA chapter. Special methods for data transformations Since data objects in R lose all user specified attributes when e.g. we transpose them or taking a subset it was decided to write several methods, which process attributes correctly. They also adjust indices of excluded rows and columns when user takes a subset or merge two data objects together. When data matrix is transposed the corresponding method will switch the x- and y- attributes. All methods with a brief description are listed in the table below (including the ones already introduces). Method Description mda.show(data) Show data object without excluded elements mda.exclrows(data, ind) Exclude (hide) rows from a data object mda.exclcols(data, ind) Exclude (hide) columns from a data object mda.inclrows(data, ind) Include (make visible) rows in a data object mda.inclcols(data, ind) Include (make visible) columns in a data object mda.t(data) Transpose data object mda.cbind(data1, data2, ...) Merge several datasets by columns mda.rbind(data1, data2, ...) Merge several datasets by rows mda.subset(data1, subset, select) Take a subset of data object (subset is numeric indices, names or logical values for rows, select — the same for columns) attrs = mda.getattr(data) Return all user specific attributes from an object data = mda.getattr(data, attrs) Assign user specific attributes to an object Data frames with factors All methods, implemented in the package, work with matrices, therefore, if a user provides data values as data frame, it is converted to matrix. From version 0.8.0 it is also possible to provide data frames with one or several factor columns. In this case all factors will be converted to dummy variables with values –1 and +1. You can also do it manually, by using function prep.df2mat() as this is shown in an example below. Let us first crate a simple data with a factor column. h = c(180, 175, 165, 190, 188) c = c(&#39;Gray&#39;, &#39;Green&#39;, &#39;Gray&#39;, &#39;Green&#39;, &#39;Blue&#39;) d = data.frame(Height = h, Eye.color = c) show(d) ## Height Eye.color ## 1 180 Gray ## 2 175 Green ## 3 165 Gray ## 4 190 Green ## 5 188 Blue And this is the result of converting it to a matrix. d.mat = mda.df2mat(d) show(d.mat) ## Height Eye.color.Blue Eye.color.Gray ## [1,] 180 0 1 ## [2,] 175 0 0 ## [3,] 165 0 1 ## [4,] 190 0 0 ## [5,] 188 1 0 The number of dummy variables is always the number of levels minus one. It is important to have level labels in all factor columns of the same data frame unique, as they are used for names of the dummy variables. If a factor is hidden it will be just converted to numeric values and remain excluded from modelling. "],
["simple-plots.html", "Simple plots", " Simple plots As it was already mentioned, mdatools has its own functions for plotting with several extra options not available in basic plot tools. These functions are used to make all plots in the models and results (e.g. scores, loadings, predictions, etc.) therefore it can be useful to spend some time and learn the new features (e.g. coloring data points with a vector of values or using manual ticks for axes). But if you are going to make all plots manually (e.g. using ggplot2) you can skip this and the next sections. In this section we will look at how to make simple plots from your data objects. Simple plots are scatter (type = 'p'), line (type = 'l'), line-scatter (type = 'b'), bar (type = 'h') or errorbar (type = 'e') plots made for a one set of objects. All plots can be created using the same method mdaplot() by providing a whole dataset as a main argument. Depending on a plot type, the method “treats” the data values differently. This table below contains a list of parameters for mdaplot(), which are not available for traditional R plots. In this section we will describe most of the details using simple examples. Parameter Description cgroup a vector of values (same as number of rows in data) used to colorize plot objects with a color gradient colmap colormap for the color gradient (possible values are 'default', 'gray' or a vector with colors) show.colorbar when color grouping is used, mdaplot() shows a colorbar legend, this parameter allows to turn it off show.labels logical parameter showing labels beside plot objects (points, lines, bars, etc). labels parameter telling what to use as labels (by default row names, but can also be indices or manual values) lab.col color for the labels lab.cex font size for the labels (as a scale factor) xticks vector with numeric values to show the x-axis ticks at yticks vector with numeric values to show the y-axis ticks at xticklabels vector with labels (numbers or text) for the x-ticks yticklabels vector with labels (numbers or text) for the y-ticks xlas an integer between 0 and 3 telling at which angle the x-tick labels have to be show ylas an integer between 0 and 3 telling at which angle the y-tick labels have to be show show.axes logical, if TRUE, function will make a new plot, if FALSE, add the plot objects to a previous one show.lines a vector with two numbers — position of horizontal and vertical lines on a plot (e.g. coordinate axes) show.grid logical, show or not a grid show.excluded logical, show or not objects corresponded to the excluded rows Scatter plots We will use people dataset for illustration how scatter plots work (see ?people for details). data(people) For scatter plots the method takes first two columns of a dataset as x and y vectors. If only one column is available (or data object is a vector), mdaplot() uses it for y-values and generate x-values as an index for each value. par(mfrow = c(2, 2)) mdaplot(people, type = &#39;p&#39;) mdaplot(people[, c(6, 7)], type = &#39;p&#39;) mdaplot(people[, 1], type = &#39;p&#39;, ylab = &#39;Height&#39;) mdaplot(people[1, ], type = &#39;p&#39;, ylab = &#39;&#39;) All parameters, available for the standard points() method will work with mdaplot() as well. Besides that, you can colorize points according to some values using a color gradient. By default, the gradient is generated using one of the diverging color schemes from colorbrewer2.org, but this can be changed using parameter colmap as it is shown below. par(mfrow = c(2, 2)) mdaplot(people, type = &#39;p&#39;, cgroup = people[, &#39;Beer&#39;]) mdaplot(people, type = &#39;p&#39;, cgroup = people[, &#39;Beer&#39;], show.colorbar = F) mdaplot(people, type = &#39;p&#39;, cgroup = people[, &#39;Beer&#39;], colmap = &#39;gray&#39;) mdaplot(people, type = &#39;p&#39;, cgroup = people[, &#39;Beer&#39;], colmap = c(&#39;red&#39;, &#39;yellow&#39;, &#39;green&#39;)) If the vector with values for color grouping is a factor, level labels will be shown on a colorbar legend. g = factor(people[, &#39;Sex&#39;], labels = c(&#39;Male&#39;, &#39;Female&#39;)) par(mfrow = c(1, 2)) mdaplot(people, type = &#39;p&#39;, cgroup = g) mdaplot(people, type = &#39;p&#39;, cgroup = g, colmap = &#39;gray&#39;) Another useful option is adding labels to the data points. By default row names will be taken for the labels but you can specify a parameter 'labels', which can be either a text ('names' or 'indices') or a vector with values to show as labels. Color and size of the labels can be adjusted. par(mfrow = c(2, 2)) mdaplot(people, type = &#39;p&#39;, show.labels = T) mdaplot(people, type = &#39;p&#39;, show.labels = T, labels = &#39;indices&#39;) mdaplot(people, type = &#39;p&#39;, show.labels = T, labels = &#39;names&#39;, lab.col = &#39;black&#39;, lab.cex = 1) mdaplot(people, type = &#39;p&#39;, show.labels = T, labels = paste(&#39;obj&#39;, 1:nrow(people))) The plots work well with the data attributes (names, axis names, etc.). Excluded rows are not shown until you specify an option show.excluded = TRUE. By the way, if you show labels as indices and some of the rows are hidden, the method takes this into account and shows indices correctly attr(people, &#39;name&#39;) = &#39;People&#39; attr(people, &#39;xaxis.name&#39;) = &#39;Parameters&#39; people = mda.exclrows(people, c(&#39;Lars&#39;, &#39;Mette&#39;, &#39;Gitte&#39;)) par(mfrow = c(2, 2)) mdaplot(people, type = &#39;p&#39;, show.labels = T) mdaplot(people, type = &#39;p&#39;, show.labels = T, labels = &#39;indices&#39;) mdaplot(people, type = &#39;p&#39;, show.labels = T, show.excluded = T) mdaplot(people, type = &#39;p&#39;, show.labels = T, show.excluded = T, labels = &#39;indices&#39;) To avoid any problems with arguments when you make a subset, use mda.subset() instead of the traditional ways. As you can see in the example below, if we take first 16 rows, information about excluded objects (as well as all other uder defined arguments, e.g. 'name') disappear and they are show in the plot as normal. But if we use mda.subset() it will take the subset without excluded rows as it is shown below. The subset can be created using logical expressions as well as indices or names of the rows. weight = people[, &#39;Weight&#39;] par(mfrow = c(2, 2)) mdaplot(people[1:16, ], show.labels = T, type = &#39;p&#39;) mdaplot(mda.subset(people, subset = 1:16), show.labels = T, type = &#39;p&#39;) mdaplot(mda.subset(people, subset = c(&#39;Lisa&#39;, &#39;Benito&#39;, &#39;Federico&#39;)), show.labels = T, type = &#39;p&#39;) mdaplot(mda.subset(people, subset = weight &gt; 70), show.labels = T, type = &#39;p&#39;) You can also manually specify axis ticks and tick labels. The labels can be rotated using parameters xlas and ylas, see the examples below. par(mfrow = c(2, 2)) mdaplot(people, xticks = c(165, 175, 185), xticklabels = c(&#39;Small&#39;, &#39;Medium&#39;, &#39;Hight&#39;)) mdaplot(people, yticks = c(55, 70, 85), yticklabels = c(&#39;Light&#39;, &#39;Medium&#39;, &#39;Heavy&#39;)) mdaplot(people, xticks = c(165, 175, 185), xticklabels = c(&#39;Small&#39;, &#39;Medium&#39;, &#39;Hight&#39;), xlas = 2, xlab = &#39;&#39;) mdaplot(people, yticks = c(55, 70, 85), yticklabels = c(&#39;Light&#39;, &#39;Medium&#39;, &#39;Heavy&#39;), ylas = 2, ylab = &#39;&#39;) If both axis labels and rotated axis ticks have to be shown, you can adjust plot margins and position of the label using par() function and mtext() for positioning axis label manually. par(mfrow = c(1, 2)) # change margin for bottom part par(mar = c(6, 4, 4, 2) + 0.1) mdaplot(people, xticks = c(165, 175, 185), xticklabels = c(&#39;Small&#39;, &#39;Medium&#39;, &#39;Hight&#39;), xlas = 2, xlab = &#39;&#39;) mtext(&#39;Height&#39;, side = 1, line = 5) # change margin for left part par(mar = c(5, 6, 4, 1) + 0.1) mdaplot(people, yticks = c(55, 70, 85), yticklabels = c(&#39;Light&#39;, &#39;Medium&#39;, &#39;Heavy&#39;), ylas = 2, ylab = &#39;&#39;) mtext(&#39;Weight&#39;, side = 2, line = 5) There are also a couple of other parameters, allowing to show/hide grid as well as show horizontal and vertical lines on the plot (axes limits will be adjusted correspondingly). par(mfrow = c(1, 2)) mdaplot(people, show.grid = F, show.lines = c(170, 65)) mdaplot(people, show.lines = c(200, NA)) Line plots When line plot is created, the mdatools() shows a line plot for every row. So if data set has more than one row, the plot will show a banch of lines having same properties (color, type, etc). This is particularly useful when working with signals and spectroscopic data. In this subsection we will use simulated UV/Vis spectra from simdata. data(simdata) spectra = simdata$spectra.c conc = simdata$conc.c[, 1] wavelength = simdata$wavelength attr(spectra, &#39;name&#39;) = &#39;UV/Vis spectra&#39; attr(spectra, &#39;xaxis.name&#39;) = &#39;Band index&#39; Here are simple examples of how to make the line plots. par(mfrow = c(2, 1)) mdaplot(spectra, type = &#39;l&#39;) mdaplot(spectra, type = &#39;l&#39;, col = &#39;darkgray&#39;, lty = 2) Most of the parameters described for scatter plots will work for the line plots as well. For example, you can colorise the lines by using a vector with some values (in the example below I use concentration of one of the chemical components). par(mfrow = c(1, 1)) mdaplot(spectra, type = &#39;l&#39;, cgroup = conc) One of the new features, appeared first in version 0.8.0, is a special attribute, allowing to provide manual x-values — 'xaxis.values' (similar parameter for y-values is 'yaxis.values'). In the example below we show the spectra using wavelength in nm and wavenumbers in inverse cm. par(mfrow = c(2, 1)) attr(spectra, &#39;xaxis.name&#39;) = expression(&#39;Wavenumbers, cm&#39;^-1) attr(spectra, &#39;xaxis.values&#39;) = 10^7/wavelength mdaplot(spectra, type = &#39;l&#39;) attr(spectra, &#39;xaxis.name&#39;) = &#39;Wavelength, nm&#39; attr(spectra, &#39;xaxis.values&#39;) = wavelength mdaplot(spectra, type = &#39;l&#39;) When you provide such data to any model methods (e.g. PCA, PLS, etc), then all variable related results (loadings, regression coefficients, etc.) will inherit this attribute and use it for making line plots. Like in scatter plots, the excluded rows will be hidden but you can show them using show.excluded = T parameter. spectra = mda.exclrows(spectra, conc &lt; 0.25) par(mfrow = c(2, 1)) mdaplot(spectra, type = &#39;l&#39;) mdaplot(spectra, type = &#39;l&#39;, show.excluded = T) You can exclude columns as well, however the excluded columns will simply not be shown. If you use manual values for x-axis, the corresponding (hidden) part will be shown just as straight lines so far. I plan to make a more smart processing of hidden columns in next versions. spectra = mda.exclcols(spectra, wavelength &gt; 280 &amp; wavelength &lt; 300) par(mfrow = c(2, 1)) mdaplot(spectra, type = &#39;l&#39;) mdaplot(spectra, type = &#39;l&#39;, show.excluded = T) Bar and errorbar plots Bar plot is perhaps the simplest as it shows values for the first row of the data as bars. Let us get back to the people data, calculate mean for all variables and show the calculated values as a bar plot (excluding column with Income as it has much bigger values comparing to the others) — in the simplest form as well as with some extra parameters. m = matrix(apply(people, 2, mean), nrow = 1) colnames(m) = colnames(people) m = mda.exclcols(m, &#39;Income&#39;) par(mfrow = c(2, 1)) mdaplot(m, type = &#39;h&#39;) mdaplot(m, type = &#39;h&#39;, xticklabels = colnames(people), col = &#39;red&#39;, show.labels = T, labels = &#39;values&#39;) Errorbar plot always expect data to have two or three rows. The first row is a origin points of the error bars, secod row is the size of the bottom part and the third row is the size of the top part. If data has only two rows the both parts will be symmetric related to the origin. In the example below we show mean and standard deviation of the people data as an error bar. d = rbind(apply(people, 2, mean), apply(people, 2, sd)) rownames(d) = c(&#39;Mean&#39;, &#39;Std&#39;) colnames(d) = colnames(people) attr(d, &#39;name&#39;) = &#39;Statistics&#39; d = mda.exclcols(d, &#39;Income&#39;) par(mfrow = c(2, 1)) mdaplot(d, type = &#39;e&#39;) mdaplot(d, type = &#39;e&#39;, xticklabels = colnames(people), col = &#39;red&#39;) All simple plots can be combined together on the same axes. In this case, first plot is created as usual and all other plots have to be created with option show.axes = F as it is shown below. It must be noted that in this case axes limits have to be set manually when creating the first plot. par(mfrow = c(2, 1)) mdaplot(m, type = &#39;h&#39;, col = &#39;lightgray&#39;, ylim = c(0, 400)) mdaplot(d, type = &#39;e&#39;, show.axes = F, pch = NA) mdaplot(m, type = &#39;b&#39;, ylim = c(0, 400)) mdaplot(d, type = &#39;e&#39;, show.axes = F) In the next section we will discuss plots for several groups of objects (rows). "],
["plots-for-groups-of-objects.html", "Plots for groups of objects", " Plots for groups of objects The package has another method for creating plots, mdaplotg(), which aims at making plots for groups of objects. It can be several groups of points, lines or bars, where evry group has its own attributes, such as color, marker, line type and width, etc. There is a simple criterion to distinguish between the simple and group plots: group plots usually need a legend and simple plots — not. The mdaplotg() method allows to do a lot of things (e.g. split data into groups, add a legend and labels, etc) much easier and this section will show most of the details. I will use the People dataset for most of the examples, so let us load it first, add some attributes, and exclude column with income. data(people) attr(people, &#39;name&#39;) = &#39;People&#39; attr(people, &#39;xaxis.name&#39;) = &#39;Parameters&#39; people = mda.exclcols(people, &#39;Income&#39;) There are three ways to provide data sets for making the group plots. Let’s discuss them first and then talk about some extra features. One matrix or data frame If dataset is a matrix or a data frame, mdaplotg() will make a line, scatter-line or a bar plot, considering every row as a separate group. This can be useful, when, for example, you want to show how explained variance depends on a number of components for calibration and test set, or how loadings for first two components look like. If you want to change any parameters, like pch, lty, lwd, col or similar you need to provide either a vector with value for each group or one value for all groups. Axis limits, ticks, ticklabels, etc. can be defined similarly to the simple plots. Here are some examples. # let&#39;s create a small subset of the people data p = mda.subset(people, subset = c(1, 2, 4), select = c(&#39;Height&#39;, &#39;Weight&#39;, &#39;Shoesize&#39;, &#39;Swim&#39;)) par(mfrow = c(2, 2)) mdaplotg(p, type = &#39;l&#39;) mdaplotg(p, type = &#39;b&#39;) mdaplotg(p, type = &#39;h&#39;, xticks = 1:4) mdaplotg(p, type = &#39;b&#39;, lty = c(1, 2, 1), col = c(&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;), pch = 1, xticklabels = colnames(p)) As you can see, mdaplotg() automatically created the legend and set colors, line parameters, etc. correctly. You can change position of the legend using same names as for basic legend() command from R, or hide it using parameter show.legend = FALSE, as it is shown below. par(mfrow = c(2, 2)) mdaplotg(p, type = &#39;l&#39;, legend.position = &#39;top&#39;) mdaplotg(p, type = &#39;b&#39;, legend.position = &#39;bottomleft&#39;) mdaplotg(p, type = &#39;h&#39;, legend.position = &#39;bottom&#39;) mdaplotg(p, type = &#39;b&#39;, show.legend = F) Group plot also allow to show labels, in this case they can be either values, names or indices of the columns. par(mfrow = c(2, 2)) mdaplotg(p, type = &#39;l&#39;, show.labels = T) mdaplotg(p, type = &#39;b&#39;, show.labels = T, labels = &#39;indices&#39;) mdaplotg(p, type = &#39;h&#39;, show.labels = T, labels = &#39;values&#39;) mdaplotg(p, type = &#39;b&#39;, show.labels = T, labels = &#39;values&#39;) List with matrices or data frames In this case every element of the list will be treated as a separate group. This way allow to make scatter plots as well and line plots with several line in a group. Barplot can be also made but in this case first row from each datasets will be used. If you use names when create the list, the names will be taken as legend labels, otherwise method will look at attribute 'name' for each data set. In the example below we split People data to males and females and show the group plots. sex = people[, &#39;Sex&#39;] m = mda.subset(people, subset = sex == -1) f = mda.subset(people, subset = sex == 1) d = list(male = m, female = f) par(mfrow = c(2, 2)) mdaplotg(d, type = &#39;p&#39;) mdaplotg(d, type = &#39;b&#39;) mdaplotg(d, type = &#39;h&#39;) mdaplotg(d, type = &#39;b&#39;, lty = c(1, 2), col = c(&#39;red&#39;, &#39;blue&#39;), pch = 1) Most of the things described in the previous subsection will work similarly for this case. We will just add a bit more details on how labels and excluded rows are processed for the scatter plots. By default labels are row names or indices. In mdaplotg() you can not provide vector with manual values, so the best way to change them is to assign them as the row names. Indices are unique within each group, so if you have, e.g. three groups of points, there will be three points with index “1”, three with “2”, etc. The points from excluded rows are shown similarly to mdaplot(). #let&#39;s exclude some rows m = mda.exclrows(m, c(1, 3, 5)) f = mda.exclrows(f, c(2, 4, 6)) d = list(male = m, female = f) par(mfrow = c(2, 2)) mdaplotg(d, type = &#39;p&#39;, legend.position = &#39;topleft&#39;, show.labels = T) mdaplotg(d, type = &#39;p&#39;, legend.position = &#39;topleft&#39;, show.labels = T, show.excluded = T) mdaplotg(d, type = &#39;p&#39;, legend.position = &#39;topleft&#39;, show.labels = T, labels = &#39;indices&#39;) mdaplotg(d, type = &#39;p&#39;, legend.position = &#39;topleft&#39;, show.labels = T, labels = &#39;indices&#39;, show.excluded = T) Working with excluding columns is a bit more difficult in case of the group plots, as, in practice, different groups might have different columns excluded. To avoid any issues it is recommended to assign the attribute xaxis.values in this case even if the values are just order numbers (1, 2, 3, …). attr(m, &#39;xaxis.values&#39;) = 1:12 attr(f, &#39;xaxis.values&#39;) = 1:12 m = mda.exclcols(m, c(&#39;Hairleng&#39;, &#39;Beer&#39;)) f = mda.exclcols(f, c(&#39;Weight&#39;, &#39;Shoesize&#39;)) d = list(male = m, female = f) par(mfrow = c(1, 2)) mdaplotg(d, type = &#39;b&#39;, legend.position = &#39;topleft&#39;, show.labels = T) mdaplotg(d, type = &#39;b&#39;, legend.position = &#39;topleft&#39;, show.labels = T, show.excluded = T) As you can note, the behavior in this case is similar to what we have seen for the mdaplot() — the excluded columns are simply ignored and the points before and after are connected with a straight line. Use factors to split a dataset into groups One more way to split data set into groups is to provide one or several factor columns using argument groupby. In this case mdaplotg() will find all combinations of the factor levels and split rows of dataset to the corresponding groups. In the example below we use variables Region and Sex to make plots for four groups. sex = factor(people[, &#39;Sex&#39;], labels = c(&#39;M&#39;, &#39;F&#39;)) reg = factor(people[, &#39;Region&#39;], labels = c(&#39;S&#39;, &#39;M&#39;)) groups = data.frame(sex, reg) par(mfrow = c(2, 2)) mdaplotg(people, type = &#39;p&#39;, groupby = groups) mdaplotg(people, type = &#39;l&#39;, groupby = groups) mdaplotg(people, type = &#39;b&#39;, groupby = groups) mdaplotg(people, type = &#39;h&#39;, groupby = groups) All parameters, described before, will work the same way in this case. "],
["working-with-images.html", "Working with images", " Working with images From version 0.8.0 the package also supports images, however they have to be transformed into datasets. The idea is very simple, we keep information about image pixels in an unfolded form, as a matrix, and use attributes width and height to reshape the data when we need to show it as an image. There are three methods that make this procedure easier: mda.im2data(), mda.data2im() and imshow(). The first convert an image (represented as 3-way array) to a data set, second does the opposite and the third takes dataset and shows it as an image. In the code chunk below you will see several examples how the methods work. We will use a dataset image available in the package. It is a 3-way array of numbers, if you want to work with e.g. JPEG, PNG or other standard image files you can load them using specific packages (jpeg, png). data(image) # convert image to a data matrix and add some attributed d = mda.im2data(img) colnames(d) = c(&#39;Red&#39;, &#39;Green&#39;, &#39;Blue&#39;) attr(d, &#39;name&#39;) = &#39;Image&#39; # show data values mda.show(d, 10) ## Image ## ----- ## Red Green Blue ## [1,] 183 80 101 ## [2,] 191 76 105 ## [3,] 187 73 99 ## [4,] 199 81 113 ## [5,] 198 81 110 ## [6,] 197 84 114 ## [7,] 191 83 109 ## [8,] 193 83 110 ## [9,] 188 83 114 ## [10,] 172 86 115 # show separate channels and the whole image in plots par(mfrow = c(2, 2)) imshow(d, 1) imshow(d, 2) imshow(d, 3) imshow(d, 1:3) By default image for one channels is shown using jet color palette for intensities, but you can also use gray colors, palette from colorbrewer2 as well as your own. par(mfrow = c(2, 2)) imshow(d, 1) imshow(d, 1, colmap = &#39;gray&#39;) imshow(d, 1, colmap = heat.colors(256)) ## Warning in if (colmap == &quot;gray&quot;) colmap = colorRampPalette(c(&quot;#000000&quot;, : ## the condition has length &gt; 1 and only the first element will be used imshow(d, 1, colmap = colorRampPalette(c(&#39;red&#39;, &#39;green&#39;))(256)) ## Warning in if (colmap == &quot;gray&quot;) colmap = colorRampPalette(c(&quot;#000000&quot;, : ## the condition has length &gt; 1 and only the first element will be used You can work with the image values as normal dataset and show scatter, line plots, calculate statistics, etc. par(mfrow = c(1, 2)) mdaplot(d, type = &#39;p&#39;) mdaplot(d, type = &#39;l&#39;) However, it will take some time to show these plots as this image has several hundreds of thousands pixels, a faster alternative can be the use of smoothScatter() function. The mdaplot() has a wrapper for the function (type = 'd'). par(mfrow = c(1, 2)) mdaplot(d, type = &#39;d&#39;) mdaplot(mda.subset(d, select = c(&#39;Red&#39;, &#39;Blue&#39;)), type = &#39;d&#39;) Another useful thing is to set some of the pixels as background. The background pixels are removed from the image dataset physically, there is no way to get them back (in cotrast to excluded rows/pixels). It can be particularly useful when working with e.g. geocorrected hyperspectral images, where, often, many pixels have NA values and there is no need to keep them in memory. To set pixels as background you need to use method mda.setimbg() with either pixel indices or vector with logical values as it is shown below. # original size show(dim(d)) ## [1] 114000 3 # set red epixels as background and show new size d = mda.setimbg(d, d[, &#39;Red&#39;] &gt; 100) show(dim(d)) ## [1] 66471 3 # show image with background pixels par(mfrow = c(1, 2)) imshow(d, 1) imshow(d, 1:3) You can also exclude pixels as rows in the image dataset. By default the excluded pixels are not shown with imshow() function (or actually they are shown as dark gray pixels), however providing argument show.excluded = TRUE you can make them visible. Note that once you exclude any pixels you can not use method mda.setimbg(), so define the background pixels first. d = mda.exclrows(d, d[, &#39;Green&#39;] &gt; 100) par(mfrow = c(2, 2)) imshow(d, 2) imshow(d, 1:3) imshow(d, 2, show.excluded = T) imshow(d, 1:3, show.excluded = T) All image related attributes are inherited by all object/rows related results, e.g. scores, residuals, predicted values and classes, etc. This means if you provide an image to any modelling method, you can visualise the corresponding results also as an image. Some examples will be shown in chapter about PCA "],
["preprocessing.html", "Preprocessing", " Preprocessing The package has several preprocessing methods implemented, mostly for different kinds of spectral data. All functions for preprocessing starts from prefix prep. which makes them easier to find by using code completion. If dataset has excluded rows, the preprocessing methods will process them and keep them excluded in the results. However if a method needs to calculate some parameters (e.g. mean values for centering, slope and intercept for MSC) the excluded rows will not be used for these calculations. The methods work similarly with the excluded columns. In this chapter a brief description of the methods with several examples will be shown. Autoscaling Autoscaling consists of two steps. First step is centering (or, more precise, mean centering) when center of a data cloud in variable space is moved to an origin. Mathematically it is done by subtracting mean from the data values separately for every column/variable. Second step is scaling og standardization when data values are divided to standard deviation so the variables have unit variance. This autoscaling procedure (both steps) is known in statistics simply as *standardization’. You can also use arbitrary values to center or/and scale the data, in this case use sequence or vector with these values should be provided as an argument for center or scale. R has a built-in function for centering and scaling, scale(). The method prep.autoscale() is actually a wrapper for this function, which is mostly needed to set all user defined attributes to the result (all preprocessing methods will keep the attributes). Here are some examples how to use it: library(mdatools) # get data and exclude column Income data(people) # centering data1 = people data1 = prep.autoscale(data1, center = T, scale = F) # standardization data2 = people data2 = prep.autoscale(data2, center = F, scale = T) # autoscaling data3 = people data3 = prep.autoscale(data3, center = T, scale = T) # centering with median values and standardization data4 = people data4 = prep.autoscale(data4, center = apply(data4, 2, median), scale = T) par(mfrow = c(2, 2)) boxplot(data1, main = &#39;Mean centered&#39;) boxplot(data2, main = &#39;Standardized&#39;) boxplot(data3, main = &#39;Mean centered and standardized&#39;) boxplot(data4, main = &#39;Median centered and standardized&#39;) Correction of spectral baseline Baseline correction methods so far include Standard Normal Variate (SNV) and Multiplicative Scatter Correction (MSC). You can find more methods in the package baseline. SNV is a very simple procedure aiming first of all at remove additive and multiplicative scatter effects from Vis/NIR spectra. It is applied to every individual spectrum by subtracting its average and dividing its standard deviation from all spectral values. Here is an example: # load UV/Vis spectra from Simdata data(simdata) ospectra = simdata$spectra.c attr(ospectra, &#39;xaxis.values&#39;) = simdata$wavelength attr(ospectra, &#39;xaxis.name&#39;) = &#39;Wavelength, nm&#39; # apply SNV and show the spectra pspectra = prep.snv(ospectra) par(mfrow = c(2, 1)) mdaplot(ospectra, type = &#39;l&#39;, main = &#39;Original&#39;) mdaplot(pspectra, type = &#39;l&#39;, main = &#39;after SNV&#39;) Multiplicative Scatter Correction does the same as SNV but in a different way. First it calculates a mean spectrum for the whole set (mean spectrum can be also provided as an extra argument). Then, for each individual spectrum, it makes a line fit for the spectral values and the mean spectrum. The coefficients of the line, intercept and slope, are used to correct the additive and multiplicative effects correspondingly. The prep.msc() function adds the mean spectrum calculated for the original spectral data, to the attributes of the results, so it can be reused later. # apply MSC and and get the preprocessed spectra pspectra = prep.msc(ospectra) # show the result par(mfrow = c(2, 1)) mdaplot(ospectra, type = &#39;l&#39;, main = &#39;Original&#39;) mdaplot(pspectra, type = &#39;l&#39;, main = &#39;after MSC&#39;) Smoothing and derivatives Savitzky-Golay filter is used to smooth signals and calculate derivatives. The filter has three arguments: a width of the filter (width), a polynomial order (porder) and the derivative order (dorder). If the derivative order is zero (default value) only smoothing will be performed. # add random noise to the spectra nspectra = ospectra + 0.025 * matrix(rnorm(length(ospectra)), dim(ospectra)) # apply SG filter for smoothing pspectra = prep.savgol(nspectra, width = 15, porder = 1) # apply SG filter for smoothing and take a first derivative dpspectra = prep.savgol(nspectra, width = 15, porder = 1, dorder = 1) # show results par(mfrow = c(2, 2)) mdaplot(ospectra, type = &#39;l&#39;, main = &#39;Original&#39;) mdaplot(nspectra, type = &#39;l&#39;, main = &#39;Noise added&#39;) mdaplot(pspectra, type = &#39;l&#39;, main = &#39;Smoothing&#39;) mdaplot(dpspectra, type = &#39;l&#39;,main = &#39;First derivative&#39;) -->"],
["principal-component-analysis.html", "Principal component analysis", " Principal component analysis In this chapter we will discuss how to use PCA method implemented in the mdatools. Besides that, we will use PCA examples to introduce some principles, which are common for most of the other methods (e.g. PLS, SIMCA, PLS-DA, etc.) available in this package. This includes such things as model and result objects, showing performance statistics for models and results, validation, different kinds of plots, and so on. Principal component analysis is one of the methods that decompose a data matrix X into a combination of three matrices: \\(X = TP&#39; + E\\). Here \\(P\\) is a matrix with unit vectors, defined in the original variables space. The unit vectors form a new basis, which is used to project all data points into. Matrix \\(T\\) contains coordinates of the projections in the new basis and product of the two matrices, \\(TP&#39;\\) represent the coordinates of projections in original variable space. Matrix \\(E\\) contains residuals — difference between position of projected data points and their original locations. In terms of PCA, the unit-vectors defining the new coordinate space are called loadings and the coordinate axes oriented alongside the loadings are Principal Components (PC). The coordinates of data points projected to the principal components are called scores. There are several other methods, such as Projection Pursuit (PP), Independent Component Analysis (ICA) and some others, that work in a similar way and resulting in the data decomposition shown above. The principal difference among the methods is the way they find the orientation of the unit-vectors. Thus, PCA finds them as directions of maximum variance of data points. In addition to that, all PCA loadings are orthogonal to each other. The PP and ICA use other criteria for the orientation of the basis vectors and e.g. for ICA the vectors are not orthogonal. It was decided to put several methods, including ICA (and in future PP) under the PCA umbrella. First of all it was done to reduce amount of code, as the interpretation and analysis of the results, the methods return, is very similar. In order to select which method (algorithm) to use for the decomposition there is a parameter method which can be defined by a user as it will be shown in the examples below. "],
["models-and-results.html", "Models and results", " Models and results In mdatools, any method for data analysis, such as PCA, PLS regression, SIMCA classification and so on, can create two types of objects — a model and a result. Every time you build a model you get a model object. Every time you apply the model to a dataset you get a result object. Thus for PCA, the objects have classes pca and pcares correspondingly. Each object includes a list with variables (e.g. loadings for model, scores and explained variance for result) and provides a number of methods for investigation. Model calibration Let’s see how this works using a simple example — People data. We used this data when was playing with plots, it consists of 32 objects (persons from Scandinavian and Mediterranean countries, 16 male and 16 female) and 12 variables (height, weight, shoesize, annual income, beer and wine consumption and so on.). More information about the data can be found using ?people. We will first load the data matrix and split it into two subsets as following: library(mdatools) data(people) idx = seq(4, 32, 4) X.c = people[-idx, ] X.t = people[idx, ] So X.c is our calibration subset we are going to use for creating a PCA model and X.t is a subset we will apply the calibrated model to. Now let’s calibrate the model and show an information about the model object: m = pca(X.c, 7, scale = T, info = &quot;People PCA model&quot;) m = selectCompNum(m, 5) Here pca is a function that builds (calibrates) a PCA model and returns the model object. Function selectCompNum allows to select an “optimal” number of components for the model. In our case we calibrate model with 7 principal components (second argument for the function pca()) however, e.g. after investigation of explained variance we found out that 5 components is optimal. In this case we have two choices. Either recalibrate the model using 5 components or use the model that is calibrated already but “tell” the model that 5 components is the optimal number. In this case the model will keep all results calculated for 10 components but will use optimal number of components when necessary. For example when showing residuals plot for the model. Or when PCA model is used in SIMCA classification. Finally, function print prints the model object info: print(m) ## ## PCA model (class pca) ## ## ## Call: ## pca(x = X.c, ncomp = 7, scale = T, info = &quot;People PCA model&quot;) ## ## Major fields: ## $loadings - matrix with loadings ## $eigenvals - eigenvalues for components ## $ncomp - number of calculated components ## $ncomp.selected - number of selected components ## $center - values for centering data ## $scale - values for scaling data ## $cv - number of segments for cross-validation ## $alpha - significance level for Q residuals ## $calres - results (scores, etc) for calibration set As you can see there are no scores, explained variance values, residuals and so on. Because they actually are not part of a PCA model, they are results of applying the model to a calibration set. But loadings, eigenvalues, number of calculated and selected principal components, vectors for centering and scaling the data, number of segments for cross-validation (if used) and significance levels are the model fields: m$loadings[1:4, 1:4] ## Comp 1 Comp 2 Comp 3 Comp 4 ## Height -0.3792846 0.08004057 -0.06676611 0.04512380 ## Weight -0.3817929 0.08533809 -0.08527883 -0.04051629 ## Hairleng 0.3513874 -0.22676635 -0.02273504 0.01575716 ## Shoesize -0.3776985 0.12503739 -0.02117369 0.09929010 One can also notice that the model object has a particular field — calres, which is in fact a PCA result object containing results of applying the model to the calibration set. If we look at the object description we will get the following: print(m$calres) ## ## Results for PCA decomposition (class pcares) ## ## Major fields: ## $scores - matrix with score values ## $T2 - matrix with T2 distances ## $Q - matrix with Q residuals ## $ncomp.selected - selected number of components ## $expvar - explained variance for each component ## $cumexpvar - cumulative explained variance And if we want to look at scores, here is the way: m$calres$scores[1:4, 1:4] ## Comp 1 Comp 2 Comp 3 Comp 4 ## Lars -5.108742 -1.2714943 1.0765871 1.08910438 ## Peter -3.021811 -0.3163758 -0.2958259 -1.36053121 ## Rasmus -2.887335 -0.4428721 0.1231706 -1.15070563 ## Mette 1.116457 -1.3716444 -1.6344512 -0.03803356 Both model and result objects also have related functions (methods), first of all for visualizing various values (e.g. scores plot, loadings plot, etc.). Some of the functions will be discussed later in this chapter, a full list can be found in help for a proper method. The result object is also created every time you apply a model to a new data. Like in many built-in R methods, method predict() is used in this case. The first argument of the method is always a model object. Here is a PCA example (assuming we have already built the model): res = predict(m, X.t) print(res) ## ## Results for PCA decomposition (class pcares) ## ## Major fields: ## $scores - matrix with score values ## $T2 - matrix with T2 distances ## $Q - matrix with Q residuals ## $ncomp.selected - selected number of components ## $expvar - explained variance for each component ## $cumexpvar - cumulative explained variance Model validation Any model can be validated with cross-validation or/and test set validation. The validation results are, of course, represented by result objects, which are fields of a model object similar to calres, but with names cvres and testres correspondingly. Here is how to build a PCA model with full cross-validation and test set validation (we will use X.t as test data) at the same time: m = pca(X.c, 7, scale = T, cv = 1, x.test = X.t, info = &quot;PCA model&quot;) m = selectCompNum(m, 5) Parameter cv specifies options for cross-validation. If a numeric value is provided then it will be used as number of segments for random cross-validation, e.g. if cv = 2 cross-validation with two segments will be used. For full cross-validation use cv = 1 like we did in the example above (this is perhaps a bit misleading, but I keep this option for compatability). For more advanced option you can provide a list with name of cross-validation method, number of segments and number of iterations, e.g. cv = list('rand', 4, 4) for running random cross-validation with four segments and four repetitions. And here is the model object info: print(m) ## ## PCA model (class pca) ## ## ## Call: ## pca(x = X.c, ncomp = 7, scale = T, cv = 1, x.test = X.t, info = &quot;PCA model&quot;) ## ## Major fields: ## $loadings - matrix with loadings ## $eigenvals - eigenvalues for components ## $ncomp - number of calculated components ## $ncomp.selected - number of selected components ## $center - values for centering data ## $scale - values for scaling data ## $cv - number of segments for cross-validation ## $alpha - significance level for Q residuals ## $calres - results (scores, etc) for calibration set ## $cvres - results for cross-validation ## $testres - results for test set As you can see we have all three types of results now — calibration (calres), cross-validation (cvres) and test set validation (testres). Let us compare, for example, the explained variance values for the results: var = data.frame(cal = m$calres$expvar, cv = m$cvres$expvar, test = m$testres$expvar) show(round(var, 1)) ## cal cv test ## Comp 1 54.2 49.6 44.8 ## Comp 2 20.3 18.8 17.2 ## Comp 3 13.1 13.2 17.0 ## Comp 4 7.9 11.3 8.0 ## Comp 5 2.3 3.2 4.4 ## Comp 6 1.1 1.8 2.4 ## Comp 7 0.5 0.7 0.7 Every model and every result has a method summary(), which shows some statistics for evaluation of a model performance. Here are some examples. summary(m) ## ## PCA model (class pca) summary ## ## Info: ## PCA model ## ## Eigvals Expvar Cumexpvar ## Comp 1 6.509 54.24 54.24 ## Comp 2 2.434 20.28 74.52 ## Comp 3 1.572 13.10 87.62 ## Comp 4 0.946 7.88 95.51 ## Comp 5 0.272 2.27 97.77 ## Comp 6 0.137 1.14 98.92 ## Comp 7 0.058 0.48 99.39 summary(m$calres) ## ## Summary for PCA results ## ## Selected components: 5 ## ## Expvar Cumexpvar ## Comp 1 54.24 54.24 ## Comp 2 20.28 74.52 ## Comp 3 13.10 87.62 ## Comp 4 7.88 95.51 ## Comp 5 2.27 97.77 ## Comp 6 1.14 98.92 ## Comp 7 0.48 99.39 The same methodology is used for any other method, e.g. PLS or SIMCA. In the next section we will look at how to use plotting functions for models and results. "],
["plotting-methods.html", "Plotting methods", " Plotting methods First of all you can use the methods mdaplot() and mdaplotg() (or any others, e.g. ggplot2) for easy visualisation the results as they all available as matrices with proper names, attributes, etc. In the example below we create several scores and loadings plots. Here I assume that the last model you have created was the one with test set and cross-validation. par(mfrow = c(1, 2)) mdaplot(m$calres$scores, type = &#39;p&#39;, show.labels = T, show.lines = c(0, 0)) mdaplot(m$loadings, type = &#39;p&#39;, show.labels = T, show.lines = c(0, 0)) To simplify this routine, every model and result class also has a number of functions for visualization. Thus for PCA the function list includes scores and loadings plots, explained variance and cumulative explained variance plots, T2 vs. Q residuals and many others. A function that does the same for different models and results has always the same name. For example plotPredictions will show predicted vs. measured plot for PLS model and PLS result, MLR model and MLR result, PCR model and PCR result and so on. The first argument must always be either a model or a result object. The major difference between plots for model and plots for result is following. A plot for result always shows one set of data objects — one set of points, lines or bars. For example predicted vs. measured values for calibration set or scores values for test set and so on. For such plots method mdaplot() is used and you can provide any arguments, available for this method (e.g. color group scores for calibration results). And a plot for a model in most cases shows several sets of data objects, e.g. predicted values for calibration and validation. In this case, a corresponding method uses mdaplotg() and therefore you can adjust the plot using arguments described for this method. Here are some examples for results: par(mfrow = c(2, 2)) plotScores(m$calres, show.labels = T) plotScores(m$calres, c(1, 3), pch = 18, cgroup = X.c[, &#39;Income&#39;], show.labels = T, labels = &#39;indices&#39;) plotResiduals(m$calres, show.labels = T, cgroup = X.c[, &#39;Weight&#39;]) plotVariance(m$calres, type = &#39;h&#39;, show.labels = T, labels = &#39;values&#39;) The color grouping option is not available for the group (model) plots as colors are used there to underline the groups. Now let’s look at similar plots (plus loadings) for a model. par(mfrow = c(2, 2)) plotScores(m, c(1, 3), show.labels = T) plotLoadings(m, c(1, 3), show.labels = T) plotResiduals(m, col = c(&#39;red&#39;, &#39;green&#39;, &#39;blue&#39;)) plotVariance(m, type = &#39;h&#39;, show.labels = T, labels = &#39;values&#39;) Method plot() shows the main four PCA plots as a model (or results) overview. plot(m, show.labels = T) You do not have to care about labels, names, legend and so on, however if necessary you can always change almost anything. See full list of methods available for PCA by ?pca and ?pcares. "],
["support-for-images.html", "Support for images", " Support for images As it was described before, images can be used as a source of data for any methods. In this case the results, related to objects/pixels will inherit all necessary attributes and can be show as images as well. In the example below we make a PCA model for the image data from the package and show scores and residuals. data(image) X = mda.im2data(img) m = pca(X) par(mfrow = c(2, 2)) imshow(m$calres$scores) imshow(m$calres$Q) imshow(m$calres$scores, 2) imshow(m$calres$Q, 2) "],
["excluded-rows-and-columns.html", "Excluded rows and columns", " Excluded rows and columns PCA implementation as well as any other method in mdatools can work with excluded rows and columns. The excluded rows are not used for creating a model and calculaiton of model’s and results’ performance (e.g. explained variance). However main results (for PCA scores and residuals) are calculated for these rows as well and set hidden. You can always e.g. show scores for excluded objects by using show.excluded = FALSE. The excluded columns are not used for any calculations either, the corresponding results (e.g. loadings or regression coefficients) will have zero values for such columns and be hidden. Here is a simple example. data(people) people = mda.exclrows(people, c(&#39;Lars&#39;, &#39;Federico&#39;)) people = mda.exclcols(people, c(&#39;Income&#39;)) m = pca(people, 5, scale = T) par(mfrow = c(2, 2)) plotScores(m, show.labels = T) plotScores(m, show.excluded = T, show.labels = T) plotResiduals(m, show.excluded = T, show.labels = T) plotLoadings(m, show.labels = T) # show matrix with loadings (look at row Income and attribute &quot;exclrows&quot;) show(m$loadings) ## Comp 1 Comp 2 Comp 3 Comp 4 Comp 5 ## Height -0.386393327 0.10697019 -0.004829174 0.12693029 -0.13128331 ## Weight -0.391013398 0.07820097 0.051916032 0.04049593 -0.14757465 ## Hairleng 0.350435073 -0.11623295 -0.103852349 -0.04969503 -0.73669997 ## Shoesize -0.385424793 0.13805817 -0.069172117 0.01049098 -0.17075488 ## Age -0.103466285 0.18964288 -0.337243182 -0.89254403 -0.02998028 ## Income 0.000000000 0.00000000 0.000000000 0.00000000 0.00000000 ## Beer -0.317356319 -0.38259695 0.044338872 -0.03908064 -0.21419831 ## Wine 0.140711271 0.57861817 -0.059833970 0.12347379 -0.41488773 ## Sex 0.364537185 -0.23838610 0.010818891 0.04025631 -0.18263577 ## Swim -0.377470722 0.04330411 0.008151288 0.18149268 -0.30163601 ## Region 0.140581701 0.60435183 0.040969200 0.15147464 0.17857614 ## IQ 0.009849911 0.09372132 0.927669306 -0.32978247 -0.11815762 ## attr(,&quot;exclrows&quot;) ## [1] 6 ## attr(,&quot;name&quot;) ## [1] &quot;Loadings&quot; ## attr(,&quot;xaxis.name&quot;) ## [1] &quot;Components&quot; Such behavior will help to exclude and include rows and columns interactively, when GUI add-in for mdatools() is available. -->"]
]
