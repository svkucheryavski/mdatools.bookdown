# Partial least squares regression {-#pls}

Partial least squares regression (PLS) is a linear regression method, which uses principles similar to PCA: data is decomposed using latent variables. Because in this case we have two datasets, matrix with predictors ($\mathbf{X}$) and matrix with responses ($\mathbf{Y}$) we do decomposition for both, computing scores, loadings and residuals: $\mathbf{X} = \mathbf{TP}^\mathrm{T} + \mathbf{E}_x$, $\mathbf{Y} = \mathbf{UQ}^\mathrm{T} + \mathbf{E}_y$. In addition to that, orientation of latent variables in PLS is selected to maximize the covariance between the X-scores, $\mathbf{T}$, and Y-scores $\mathbf{U}$. This approach makes possible to work with datasets where more traditional Multiple Linear Regression fails — when number of variables exceeds number of observations and when X-variables are mutually correlated. But, at the end, PLS-model is a linear model, where response value is just a linear combination of predictors, so the main outcome is a vector with regression coefficients.

There are two main algorithms for PLS, *NIPALS* and *SIMPLS*, in the *mdatools* only the last one is implemented. PLS model and PLS results objects have a lot of properties and performance statistics, which can be visualized via plots. Besides that, there is also a possibility to compute selectivity ratio (SR) and VIP scores, which can be used for selection of most important variables. Another additional option is a randomization test which helps to select optimal number of components. We will discuss most of the methods in this chapter and you can get the full list using `?pls`.

## Models and results {-#pls--models-and-results}

Like we discussed for [PCA](#pca), *matools*  creates two types of objects — a model and a result. Every time you build a  PLS model you get a *model object*. Every time you apply the model to a dataset you get a *result object*. For PLS, the objects have classes `pls` and `plsres` correspondingly.

### Model calibration {-}

Let's use the same *People* data and create a PLS-model for prediction of *Shoesize* (column number four) using other 11 variables as predictors. As usual, we start with preparing datasets (we will also split the data into calibration and test subsets):

```{r}
library(mdatools)
data(people)

idx = seq(4, 32, 4)
Xc = people[-idx, -4]
yc = people[-idx, 4, drop = FALSE]
Xt = people[idx, -4]
yt = people[idx, 4, drop = FALSE]
```

So `Xc` and `yc` are predictors and response values for calibration subset. Now let's calibrate the model and show an information about the model object:

```{r}
m = pls(Xc, yc, 7, scale = TRUE, info = "Shoesize prediction model")
```
You can notice that the calibration succeeded but there is also a warning about lack of validation results. For supervised models, which have complexity parameter (in this case — number of components), doing proper validation is important as it helps to find the optimal complexity. When you calibrate PLS model the calibration also tries to find the optimal number (details will be discussed later in this chapter) and this needs some validation. How to do proper validation of PLS models is discussed in the next section.

Here is an info for the model object:

```{r}
print(m)
```

As expected, we see loadings for predictors and responses, matrix with weights, and a special object (`regcoeffs`) for regression coefficients.

#### Result object {-}

Similar to PCA, model object contains list with result objects (`res`), obtained using calibration set (`cal`), cross-validation (`cv`) and test set validation (`test`). All three have class `plsres`, here is how `res$cal` looks like:

```{r}
print(m$res$cal)
```

The `xdecomp` and `ydecomp` are objects similar to `pcares`, they contain scores, residuals and variances for decomposition of X and Y correspondingly.

```{r}
print(m$res$cal$xdecomp)
```

Other fields are mostly various performance statistics, including slope, coefficient of determination (R^2^), bias, and root mean squared error (RMSE). Besides that, the results also include reference y-values and array with predicted y-values. The array has dimension *nObjects x nComponents x nResponses*.

PLS predictions for a new set can be obtained using method `predict`:

```{r}
res = predict(m, Xt, yt)
print(res)
```

If reference y-values are not provided to `predict()` function, then all predictions are computed anyway, but performance statistics (and corresponding plot) will be not be available.

## Validation {-}

Validation of PLS (and PLS-DA) models can be done by one of the following methods:

1. Using separate validation/test set
2. Uisng Procrustes cross-validation
3. Using conventional cross-validation

Below we will show each of the method in detail. For all examples we will use *Simdata* with UV/VIS spectra of mixtures of three carbonhydrates, this dataset has been already used for examples in Preprocessing and PCA chapters. It contains spectra and reference concentrations for two sets of measurements.

### Using validation/test set {-}

In case if you have a dedicated validation/test set, you need to provide it as two separate matrices — one for predictors (parameter `x.test`) and one for response (`y.test`) values. The rest will be done automatically and you will see the results for test set in all plots and summary tables.

The example below shows how to create such model for *Simdata* (here we will make PLS1 model for prediction of concentration of the first mixture component — first column of the concentration matrix):

```{r}
data(simdata)
Xc = simdata$spectra.c
yc = simdata$conc.c[, 1]
Xt = simdata$spectra.t
yt = simdata$conc.t[, 1]

m = pls(Xc, yc, 10, x.test = Xt, y.test = yt)
summary(m)
```

As you can see, in this case we do not have a warning about absence of validation results as we have seen in the example above. Summary information is avaiable for optimal number of components for both calibration and test set. Model was able to detect that 3 components is optimal despite the provided number of 10 components. How this selection is done is described later in this chapter, meanwhile let's look at the plot overview:

```{r, fig.width = 9, fig.height = 9}
plot(m)
```

Distance plot, RMSE and plot with predicted and measured y-values have points for both calibration and test set. The RMSE plot shows clearly that after third component the model becomes overfitted as error for the test set is increasing.

As mentioned, method `summary()` for validated model shows performance statistics calculated using optimal number of components for each of the results (calibration and test set validation in our case).

If you want more details run `summary()` for one of the result objects.

```{r}
summary(m$res$test)
```

In this case, the statistics are shown for all available components and explained variance for individual components is added.

### Using Procrustes cross-validation {-}

As in case of PCA, you can also generate the validation set using Procrustes cross-validation. Here I assume that you already have package `pcv` instealled and read about the method (or at least tried it with PCA examples).

Let's create the PV-set first:

```{r}
library(pcv)
Xpv = pcvpls(Xc, yc, 20, cv = list("ven", 4))
```

In this case we generate only predictors, the response values will be the same as for the calibration set. Now we can create a model similar to what we did for dedicated validation set (note that we provide `yc` as values for the `y.test` argument in this case):

```{r}
m2 = pls(Xc, yc, 10, x.test = Xpv, y.test = yc)
summary(m2)
```

As you can see in this case PLS also selected 3 components as optimal. Here is the overview of the model.

```{r, fig.width = 9, fig.height = 9}
plot(m2)
```

The behaviour of RMSE values is quite similar to the one we got for the test set.

### Using conventional cross-validation {-}

Alternatively you can use conventional cross-validation as well. But in this case only figures of merit (e.g. RMSE and R2 values) will be computed, as computing variances and distances can not be done correctly. This is enough though to optimize the parameters of your model (number of components, preprocessing, variable selection) and then apply the final optimized model to the test set.

In this case you do not need to provide any extra data, but just set cross-validation parameter `cv` similar to what you do when generate PV-set:

```{r}
m3 = pls(Xc, yc, 10, cv = list("ven", 4))
summary(m3)
```


```{r, fig.width = 9, fig.height = 9}
plot(m3)
```

As you can see, all results are very similar to what you got with PV-set, however some of the outcomes (e.g. distances, explained variance, etc) are not available.

In the examples above we set arameter `cv` to `list("ven", 4)`. But in fact it can be a number, a list or a vector with segment values manually assigned for each measurement.

If it is a number, it will be used as number of segments for random cross-validation, e.g. if `cv = 2` cross-validation with two segments will be carried out with measurements being randomly split into the segments. For full cross-validation use `cv = 1` like in the example above. This option is a bit confusing, logically we have to set `cv` equal to number of measurements in the calibration set. But this option was used many years ago when the package was created and it is kept for backward compatibility.

For more advanced selection you can provide a list with name of cross-validation method, number of segments and number of iterations, e.g. `cv = list("rand", 4, 4)` for running random cross-validation with four segments and four repetitions or `cv = list("ven", 8)` for systematic split into eight segments (*venetian blinds*). In case of venetian splits, the measurements will be automatically ordered from smallest to largest response (y) values.

Finally, you can also provide a vector with manual slits. For example, if you create a model with the following value for `cv` parameter: `cv = rep(1:4, length.out = nrow(Xc))` the vector of values you provide will look as `1 2 3 4 1 2 3 4 1 2 ...`, which corresponds to systematic splits but without ordering y-values, so it will use the current order of measurements (rows) in the data. Use this option if you have specific requirements for cross-validation and the implemented methods do not meet them.

#### Local and global CV scope {-}

From *0.14.1* it is possible to set how cross-validation subsets are centred and scaled inside the cross-validation loop. This is regulated by a new parameter `cv.scope` in methods `pls()`, `plsda()`and `ipls()`.

Cross-validation splits the original training set into local calibration and validation sets inside the loop. For example, if you have dataset with 100 objects/observations and use cross-validation with four segments (with random or systematic splits), in each iteration 25 objects will be taken as local validation set and remaining 75 objects will for a local calibration set.

If `cv.scope` parameter is set to `"local"`, then mean values for centreing and standard deviation values for scaling (if this option is selected) will be computed based on the local set with 75 objects in the example below. So, every local model will have its own center in the variable space.

If `cv.scope` parameter is set to `"global"`, then the mean values and standard deviation values will be computed for the original (global) training set hence all local models will have the same center as the global model.

The default value is `"local"` and this was also the way `mdatools` worked before this option was introduced in *0.14.1*. The small example below shows how both options work (it is assumed that you already have `Xc` and `yc` variables from the previos examples):

```{r, fig.width = 9, fig.height = 5}

m1 = pls(Xc, yc, 7, cv = list("ven", 4), cv.scope = "local")
m2 = pls(Xc, yc, 7, cv = list("ven", 4), cv.scope = "global")

par(mfrow = c(1, 2))
plotRMSE(m1$res$cv, type = "h", show.labels = TRUE, main = "Local cv-scope")
plotRMSE(m2$res$cv, type = "h", show.labels = TRUE, main = "Global cv-scope")
```
As you can see the cross-validation results are very similar, however RMSE values for locally autoscaled models tend to be a bit higher as expected.

### Selection of optimal number of components {-}

When you create a PLS model, it tries to select optimal number of components automatically (which, of course, you can always change later). To do that, the method uses RMSE values, calculated for different number of components and validation predictions. So, if we do not use any validation, it warns us about this as it was mentioned in the beginning of this chapter.

If validation (any of the three methods described above) was used, there are two different ways/criteria for automatic selection of the components best on validation results. One is using first local minimum on the RMSE plot and second is so called Wold criterion, based on a ratio between PRESS values for current and next component.

You can select which criterion to use by specifying parameter `ncomp.selcrit` (either `'min'` or `'wold'`) as it is shown below (here we use cross-validation).

```{r}
m1 = pls(Xc, yc, 7, cv = list("ven", 4), ncomp.selcrit = "min")
show(m1$ncomp.selected)

m2 = pls(Xc, yc, 7, cv = list("ven", 4), ncomp.selcrit = "wold")
show(m2$ncomp.selected)
```

Well, in this case both pointed on the same number, 3, but sometimes they give different solutions.

And here are the RMSE plots (they are identical of course), where you can see how error depends on number of components for both calibration set and cross-validated predictions. Apparently the minimum for cross-validation error (RMSECV) is indeed at 3:

```{r, fig.width = 9, fig.height = 5}
par(mfrow = c(1, 2))
plotRMSE(m1)
plotRMSE(m2)
```

Another useful plot in this case is a plot which shows ratio between cross-validated RMSE values, RMSECV, and the calibrated ones, RMSEC. You can see an example in the figure below and read more about this plot in [blog post](https://eigenvector.com/%EF%BF%BCevaluating-models-hating-on-r-squared/) by Barry M. Wise.


```{r, fig.width = 9, fig.height = 5}
par(mfrow = c(1, 2))
plotRMSERatio(m1)
plotRMSERatio(m2, pch = 1, col = "red")
```

## Regression coefficients {-}

First of all let us re-create the PLS model for prediction of Showsize from People data we used at the beginning of this chapter:


```{r}
library(mdatools)
data(people)

idx = seq(4, 32, 4)
Xc = people[-idx, -4]
yc = people[-idx, 4, drop = FALSE]
Xt = people[idx, -4]
yt = people[idx, 4, drop = FALSE]
```

But this time I will create the model using full cross-validation:

```{r}
m = pls(Xc, yc, 7, scale = TRUE, cv = 1, info = "Shoesize prediction model")
```

The values for regression coefficients are available in `m$coeffs$values`, it is an array with dimension *nVariables x nComponents x nPredictors*. The reason to use the object instead of just an array is mainly for being able to get and plot regression coefficients for different methods. Besides that, it is possible to calculate confidence intervals and other statistics for the coefficients using Jack-Knife method (will be shown later), which produces extra entities.

The regression coefficients can be shown as plot using either function `plotRegcoeffs()` for the PLS model object or function `plot()` for the object with regression coefficients. You need to specify for which predictor (if you have more than one y-variable) and which number of components you want to see the coefficients for. By default it shows values for the optimal number of components and first y-variable as it is shown on example below.

```{r, fig.width = 9, fig.height = 9}

par(mfrow = c(2, 2))
plotRegcoeffs(m)
plotRegcoeffs(m, ncomp = 2)
plot(m$coeffs, ncomp = 3, type = "b", show.labels = TRUE)
plot(m$coeffs, ncomp = 2)
```

The model keeps regression coefficients, calculated for centered and standardized data, without intercept, etc. Here are the values for three PLS components.

```{r}
show(m$coeffs$values[, 3, 1])
```

You can see a summary for the regression coefficients object by calling function `summary()` for the object `m$coeffs` like it is show below. By default it shows only estimated regression coefficients for the selected y-variable and number of components.

However, if you use cross-validation, [Jack-Knifing](https://en.wikipedia.org/wiki/Jackknife_resampling) method will be used to compute some statistics, including standard error, p-value (for test if the coefficient is equal to zero in population) and confidence interval. All statistics in this case will be shown automatically with `summary()` as you can see below.

```{r}
summary(m$coeffs)
```

You can also get the corrected coefficients, which can be applied directly to the raw data (without centering and standardization), by using method `getRegcoeffs()`:

```{r}
show(getRegcoeffs(m, ncomp = 3))
```


## Plotting methods {-#pls--plotting-methods}

Plotting methods, again, work similar to PCA, so in this section we will look more detailed at the available methods instead of on how to customize them. PLS has a lot of different results and much  more possible plots. Here is a list of methods, which will work both for a model and for a particular results.

Plotting methods for summary statistics.

----------------------------------------------------------------------------------------------------
Method                                  Description
------------------------------------    ------------------------------------------------------------
`plotRMSE(obj, ny, ...)`                RMSE values vs. number of components in a model

`plotRMSERatio(obj, ny, ...)`           RMSECV/RMSEC ratio values vs. RMSECV in a model

`plotXVariance(obj, ...)`               explained variance for X decomposition for each component

`plotXCumVariance(obj, ...)`            same as above but for cumulative variance

`plotYVariance(obj, ...)`               explained variance for Y decomposition for each component

`plotYCumVariance(obj, ...)`            same as above but for cumulative variance

----------------------------------------------------------------------------------------------------


Plotting methods for objects.

----------------------------------------------------------------------------------------------------
Method                                         Description
----------------------------------------       -----------------------------------------------------
`plotPredictions(obj, ny, ncomp, ...)`         plot with predicted vs. measured (reference) y-values

`plotXScores(obj, comp, ...)`                  scores for decomposition of X (similar to PCA plot)

`plotYScores(obj, comp, ...)`                  scores for decomposition of Y (similar to PCA plot)

`plotXResiduals(obj, ncomp, ...)`              distance plot for decomposition of X (similar to PCA)

`plotYResiduals(obj, ncomp, ...)`              distance plot for decomposition of Y

`plotXYResiduals(obj, ncomp, ...)`             distance plot for both X and Y decomposition

`plotXYScores(obj, ncomp, ...)`                Y-scores vs. X-scores for a particular PLS component.

----------------------------------------------------------------------------------------------------

Parameter `obj` is either a model or a result object and it is the only mandatory argument for the plots. All other parameters have reasonable default values. Parameter `ny` is used to specify which y-variable you want to see a plot for (if $\mathbf{Y}$ is multivariate). You can also provide any parameter from `mdaplot()` or `mdaplotg()` thus change limits or labels for axis, main title, colors, line and marker style etc.

Parameter `comp` allows to provide a number of selected components (one or several) to show the plot
for, while parameter `ncomp` assumes that only one number is expected (number of components in a model or particular individual component). So if e.g. you create model for five components and selected three as optimal, you can also see, for example, prediction plot for having only one or for components in your model.

Here is an example for creating prediction plot for `m` model (left for automatically selected number of components, right for user specified value):

```{r, fig.width = 9, fig.height = 5}
par(mfrow = c(1, 2))
plotPredictions(m)
plotPredictions(m, ncomp = 1)
```

By the way, when `plotPredictions()` is made for results object, you can show performance statistics on the plot:

```{r, fig.width = 9, fig.height = 9}
par(mfrow = c(2, 2))
plotPredictions(m$res$cal)
plotPredictions(m$res$cal, ncomp = 2)
plotPredictions(m$res$cal, show.stat = TRUE)
plotPredictions(m$res$cal, ncomp = 2, show.stat = TRUE)
```

The plots for variables are available only for a model object and include:

----------------------------------------------------------------------------------------------------
Methods                                            Description
----------------------------------------------     -------------------------------------------------
`plotXLoadings(obj, comp)`                         loadings plot for decomposition of X

`plotXYLoadings(obj, comp)`                        loadings plot for both X and Y decomposition

`plotWeights(obj, comp)`                           plot with weights (*W*) for PLS decomposition

`plotRegcoeffs(obj, ny, ncomp)`                    plot with regression coefficients

`plotVIPScores(obj, ny)`                           VIP scores plot

`plotSelectivityRatio(obj, ny, ncomp)`             Selectivity ratio plot

----------------------------------------------------------------------------------------------------

And, of course, both model and result objects have method `plot()` for giving an overview.

```{r, fig.width = 9, fig.height = 8}
plot(m)
```

### Excluding rows and columns {-}

PLS, like PCA, also can exclude rows and columns from calculations. The implementation works similar to what was [described][Excluding rows and columns] for PCA. For example, it can be useful if you have some candidates for outliers or do variable selection and do not want to remove rows and columns physically from the data matrix. In this case you can just specify two additional parameters, `exclcols` and `exclrows`, using either numbers or names of rows/columns to be excluded. You can also specify a vector with logical values (all `TRUE`s will be excluded).

The excluded rows are not used for creating a model and calculation of model's and results' performance (e.g. explained variance). However main results (for PLS — scores, predictions, distances) are calculated for these rows as well and set hidden, so you will not see them on plots. You can always e.g. show scores for excluded objects by using `show.excluded = TRUE`. It is implemented via attributes "known" for plotting methods from *mdatools* so if you use e.g. *ggplot2* you will see all points.

The excluded columns are not used for any calculations either, the corresponding results (e.g. loadings, weights or regression coefficients) will have zero values for such columns and be also hidden on plots.

## Variable selection {-#pls--variable-selection}

PLS allows to calculate several statistics, which can be used to select most important (or remove least important) variables in order to improve performance and make model simpler. The first two are VIP-scores (variables important for projection) and Selectivity ratio. All details and theory can be found e.g. [here](https://doi.org/10.1002/cem.1360).

Both parameters can be shown as plots and as vector of values for a selected y-variable. Take into account that when you make a plot for VIP scores or Selectivity ratio, the corresponding values should be computed first, which can take some time for large datasets.

Here is an example of corresponding plots.

```{r, fig.width = 9, fig.height = 9}
par(mfrow = c(2, 2))
plotVIPScores(m)
plotVIPScores(m, ncomp = 2, type = "h", show.labels = TRUE)
plotSelectivityRatio(m)
plotSelectivityRatio(m, ncomp = 2, type = "h", show.labels = TRUE)
```

To compute the values without plotting use `vipscores()` and `selratio()` functions.  In the example below, I create two other PLS models by excluding variables with VIP score or selectivity ratio below a threshold (I use 0.5 and 2 correspondingly) and show the performance for both.

```{r}
vip = vipscores(m, ncomp = 2)
m2 = pls(Xc, yc, 4, scale = T, cv = 1, exclcols = (vip < 0.5))
summary(m2)

sr = selratio(m, ncomp = 2)
m2 = pls(Xc, yc, 4, scale = T, cv = 1, exclcols = (sr < 2))
summary(m2)
```

Another way is to make an inference about regression coefficients and calculate confidence intervals and p-values for each variable. This can be done using Jack-Knife approach, when model is cross-validated using efficient number of segments (at least ten) and statistics are calculated using the distribution of regression coefficient values obtained for each step. The statistics are automatically computed when you use full cross-validation.

```{r, fig.width = 9, fig.height = 9}
mjk = pls(Xc, yc, 7, scale = TRUE, cv = 1)
```

The statistics are calculated for each y-variable and each available number of components. When you show a plot for regression coefficients, you can show the confidence intervals by using parameter `show.ci = TRUE` as shown in examples below.

```{r, fig.width = 9, fig.height = 9}
par(mfrow = c(2, 2))
plotRegcoeffs(mjk, type = "h", show.ci = TRUE, show.labels = TRUE)
plotRegcoeffs(mjk, ncomp = 2, type = "h", show.ci = TRUE, show.labels = TRUE)
plotRegcoeffs(mjk, type = "l", show.ci = TRUE, show.labels = TRUE)
plotRegcoeffs(mjk, ncomp = 2, type = "l", show.ci = TRUE, show.labels = TRUE)
```

Calling function `summary()` for regression coefficients allows to get all calculated statistics.

```{r}
summary(mjk$coeffs, ncomp = 2)
summary(mjk$coeffs, ncomp = 2, alpha = 0.01)
```

Function `getRegcoeffs()` in this case may also return corresponding t-value, standard error, p-value, and confidence interval for each of the coefficient (except intercept) if user specifies a parameter `full`. The standard error and confidence intervals are also computed for raw, non-standardized, variables (similar to coefficients).

```{r}
show(getRegcoeffs(mjk, ncomp = 2, full = TRUE))
```

It is also possible to change significance level for confidence intervals.

```{r}
show(getRegcoeffs(mjk, ncomp = 2, full = TRUE, alpha = 0.01))
```

The p-values, t-values and standard errors are stored each as a 3-way array similar to regression coefficients. The selection can be made by comparing e.g. p-values with a threshold similar to what we have done with VIP-scores and selectivity ratio.

```{r}
exclcols = mjk$coeffs$p.values[, 2, 1] > 0.05
show(exclcols)
```

Here `p.values[, 2, 1]` means values for all predictors, model with two components, first y-variable.

```{r}
newm = pls(Xc, yc, 3, scale = TRUE, cv = 1, exclcols = exclcols)
summary(newm)
show(getRegcoeffs(newm))
```

As you can see, the variables *Age*, *Income*, *Region* and *IQ* have been excluded as they are not related to the *Shoesize*, which seems to be correct.

Variable selection as well as all described above can be also carried out for PLS discriminant analysis (PLS-DA), which can be explained later in one of the next chapters.

## Distances and outlier detection {-#pls--distances-and-outliers}

### Distance plot for X-decomposition {-}

For decomposition of X-values the orthogonal and score distances are computed and treated in the same way as in PCA. The `pls()` constructor takes the three arguments for computing the critical limits, similar to PCA (`lim.type`, `alpha`, `gamma`). The default value for `lim.type` is the same as for PCA (`"ddmoments"`). Distance plot can be made using `plotXResiduals()`, it works identical to `plotResiduals()` for PCA.

Below is example on how to use the plot.

```{r, fig.width = 9, fig.height = 9}
m = pls(Xc, yc, 4, scale = TRUE, cv = 1)

par(mfrow = c(2, 2))
plotXResiduals(m)
plotXResiduals(m, ncomp = 2, log = TRUE)
plotXResiduals(m, ncomp = 2, res = list("cal" = m$res$cal))
plotXResiduals(m$res$cal)
```

PLS also has function `categorize()` which allows to identify extreme objects and outliers. However, it works in a different way and takes into account not only total distances for X-decomposition but also similar measure for decomposition of Y. This is explained in following sections.

### Distance plot for Y-decomposition and total distance {-}

The distance for Y-decomposition is calculated in a different way — as a difference between predicted and reference y-values. The difference for selected y-variable is expected to be random and normally distributed. It can be checked visually by using function `plotYResiduals()` as shown in example below. In this plot the difference is plotted against the reference value.

```{r, fig.width = 9, fig.height = 9}
par(mfrow = c(2, 2))
plotYResiduals(m)
plotYResiduals(m, ncomp = 2)
plotYResiduals(m, ncomp = 2, res = list("cal" = m$res$cal))
plotYResiduals(m$res$cal)
```

Since predictions are also computed for cross-validation, the plot shows the distance values for cross-validated results.

The distance between reference and predicted values can be used to compute orthogonal squared distance, similar to how it is done for X-decomposition, as it is shown below:

$$ z_i = \sum_{k = 1}^K (y_{ik} - \hat{y}_{ik})^2 $$

Here $y_{ik}$ is a reference value for sample $i$ and y-variable $k$ and $\hat{y}_{ik}$ is the corresponding predicted value, $K$ is the number of y-variables. Apparently, $z$ values also follow chi-square distribution similar to how [it is done in PCA][Critical limits]:

$$N_z \frac{z}{z_0} \propto \chi^2(N_z)$$

In case of PLS1 (when there is only one y-variable), $N_z = 1$. Otherwise, both $z_0$ and $N_z$ are computed using [data driven approach][Data driven approach], similar to $q_0$, $N_q$, $h_0$, $N_h$, using either classical (based on moments) or robust approach.

Full distance for X-decomposition, $f$ and the Y-distance, $z$ can be combined into XY total distance, $g$:

$$g = N_f\frac{f}{f_0} + N_z\frac{z}{z_0} \propto \chi^2(N_g)$$

Here $N_g = N_f + N_z$. This approach was proposed by Rodionova and Pomerantsev and described in [this paper](https://pubs.acs.org/doi/10.1021/acs.analchem.9b04611). The distances and the critical limits can be visualized using function `plotXYResiduals()` as illustrated in the next example.

The example is based on People data (prediction of shoesize), however we will first introduce two outliers. We will change response value for row #9 to 25, which is apparently too small value for shoesize of an adult person. The predictor values will be the same. Then we will change height of first person (row #1) to 125 cm, which is again quite small value. And keep the response value unchanged. After that, we create a PLS model with classic data driven approach for computing critical limits and show the XY-distance plot. Finally, that we change the method for limits to robust and show the plot again:

```{r, fig.width = 9, fig.height = 5}
# prepare data
data(people)
X = people[, -4]
y = people[, 4, drop = FALSE]

# add outliers
y[9] = 25
X[1, 1] = 125

# create model and show plots
m = pls(X, y, 4, scale = TRUE, cv = 1, lim.type = "ddmoments")

par(mfrow = c(1, 2))
plotXYResiduals(m, show.labels = TRUE, labels = "indices")

m = setDistanceLimits(m, lim.type = "ddrobust")
plotXYResiduals(m, show.labels = TRUE, labels = "indices")
```

As one can see, both classic and robust approach detect sample number 9 (with wrong y-value) as a clear outlier. Using robust approach helps to identify the second outlier as well. However, you can see that the location of the two samples is different — sample #9 has large Y-distance ($z/z_0$), while sample #1 has large full X-distance ($f/f_0$). Which is in agreement with the way we created the outliers.

The limits shown on the plot are made for the total distance, $g$. For example, to detect outliers we need to compare the total distance, $g$, with critical limit computed for predefined significance level, $\gamma$ (shown as dotted line on the plots):

$$g > \chi^{-2} \big( (1 - \gamma)^{1/I}, N_g \big) $$

The proper procedure for detection and removing outliers can be found in the [paper](https://pubs.acs.org/doi/10.1021/acs.analchem.9b04611) and will be very briefly touched upon here. The detection can be done using function `categorize()` which works similar to the same function for PCA decomposition — returns vector with categories for each measurement, as shown in an example below:

```{r}
c = categorize(m, m$res$cal)
print(c)
```

A simplified description of the recommended procedure proposed in the [mentioned paper](https://pubs.acs.org/doi/10.1021/acs.analchem.9b04611) is following:

1. Make a PLS model with robust estimator of critical limits
2. Detect outliers if any, remove them and keep the removed objects separately
3. Repeat steps 1-2 until no outliers are detected
4. Apply model to the collected outliers (use `predict`)
5. If any of them appear as regular objects, return them back to the dataset and go to step 1
6. Repeat all steps until step 5 does not reveal any regular objects
7. Make a final PLS model using classic estimator for the critical limits

If test set validation is used, the outlier detection should be done for both sets. And it is of course important to use optimal number of components, which can be identified using RMSE plot and plot for explained Y-variance.

The code chunks below show the steps for detection of outliers in the People data used in an example earlier. We start with creating PLS model for the whole data.

```{r, fig.width = 9, fig.height = 5}
# prepare data
data(people)
X = people[, -4]
y = people[, 4, drop = FALSE]

# add outliers
y[9] = 25
X[1, 1] = 125

# compute initial PLS model with all data
m = pls(X, y, 10, scale = TRUE, cv = 1, lim.type = "ddrobust")

# look at RMSE and XY-residuals plot
par(mfrow = c(1, 2))
plotRMSE(m)
plotXYResiduals(m)
```

Apparently four components selected automatically is indeed a proper value. The XY-distance plot shows that we have two outliers, let's find and remove them:

```{r, fig.width = 9, fig.height = 5}
# get row indices for outliers in calibration set
outliers = which(categorize(m, m$res$cal) == "outlier")

# keep data for outliers in separate matrices
Xo = X[outliers, , drop = FALSE]
yo = y[outliers, , drop = FALSE]

# remove the rows with outliers from the data
X = X[-outliers, , drop = FALSE]
y = y[-outliers, , drop = FALSE]

# make a new model for oulier free data
m = pls(X, y, 10, scale = TRUE, cv = 1, lim.type = "ddrobust")

# look at RMSE and XY-distance plot
par(mfrow = c(1, 2))
plotRMSE(m)
plotXYResiduals(m)
```
Again, four or three components seem to be optimal and this time no extra outliers are detected. Let's now apply the model to the two outliers found previously and see their status.

```{r, fig.width = 4.5, fig.height = 5}
res = predict(m, Xo, yo)
plotXYResiduals(m, res = list("cal" = m$res$cal, "out" = res))
```

The two objects seem now even more extreme for the model built using the outliers free data, no need to have them back. So we just need to make a final model and look at it:

```{r, fig.width = 9, fig.height = 9}
# make a new model for outlier free data and explore it
m = pls(X, y, 10, scale = TRUE, cv = 1, lim.type = "ddmoments")
summary(m)

par(mfrow = c(2, 2))
plotXYResiduals(m)
plotRegcoeffs(m, type = "h", show.labels = TRUE, show.ci = TRUE)
plotRMSE(m)
plotPredictions(m)
```


## Randomization test {-#pls--randomization-test}

Another additional option for PLS regression implemented in *mdatools* is randomization test for estimation of optimal number of components. The description of the method can be found in [this paper](https://onlinelibrary.wiley.com/doi/10.1002/cem.1086). The basic idea is that for each component from 1 to `ncomp` we compute a statistic $T$, which is a covariance between X-scores and the reference Y values. After that, this procedure is repeated for randomly permuted Y-values and distribution of the statistic is obtained. A parameter `alpha` is computed to show how often the statistic $T$, calculated for permuted Y-values, is the same or higher than the same statistic, calculated for original response values without permutations.

If a component is important, then the covariance for non-permuted data should be larger than the covariance for permuted data and therefore the value for `alpha` will be quite small (there is still a small chance to get similar covariance). This makes `alpha` very similar to p-value in a statistical test.

The function `randtest()` calculates alpha for each component, the values can be observed using `summary()` or `plot()` functions. There are also several functions, allowing e.g. to show distribution of statistics and the critical value for each component.

In example of code below most of the functions are shown.

```{r}
data(people)

y = people[, 4, drop = FALSE]
X = people[, -4]

r = randtest(X, y, ncomp = 5, nperm = 1000, silent = TRUE)
summary(r)
```

As you can see, `alpha` is very small for components 2–4 and then jumps up.

```{r, fig.width = 9, fig.height = 9}
par( mfrow = c(2, 2))
plotHist(r, ncomp = 3)
plotHist(r, ncomp = 5)
plotCorr(r, ncomp = 3)
plotCorr(r, ncomp = 5)
```
